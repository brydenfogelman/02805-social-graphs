{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Week 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import requests, io, re\n",
    "import cPickle as pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing real text (from out on the inter-webs)\n",
    "\n",
    "*Exercise:* Just a couple of examples from the book: Work through the exercises NLPP1e 3.12: 6, 30.\n",
    "\n",
    "** Describe the class of strings matched by the following regular expressions. **\n",
    "1. [a-zA-Z]+ -> all words that can be in lowercase or capital letters\n",
    "2. [A-Z][a-z]* -> A single capital letter, or a word starting by a capital letter and followed by lowercase letters (a word at the begining of the sentence)\n",
    "3. p[aeiou]{,2}t -> All words starting by a p and ending by a t, with between 0 and 2 characters from aeio in between\n",
    "4. \\d+(\\.\\d+)? -> Captures the decimal part of a number, with the leading point\n",
    "5. ([^aeiou][aeiou][^aeiou])* -> Any word containing a letter in aeiou, that isn't directly precedeed by any of those letters, and not followed by any of those letters. \n",
    "6. \\w+|[^\\w\\s]+ -> Match literally any character (either a word or anything that is not a word). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and see if you observe any differences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'The', u'gold', u'dollar', u'wa', u'a', u'coin', u'struck', u'as', u'a', u'regular', u'issu', u'by', u'the', u'Unit', u'State', u'Bureau', u'of', u'the', u'Mint', u'from', u'1849', u'to', u'1889', u'.', u'The', u'coin', u'had', u'three', u'type', u'over', u'it', u'lifetim', u',', u'all', u'design', u'by', u'Mint', u'Chief', u'Engrav', u'Jame', u'B.', u'Longacr', u'.', u'The', u'Type', u'1', u'issu', u'had', u'the', u'smallest', u'diamet', u'of', u'ani', u'Unit', u'State', u'coin', u'ever', u'mint', u'.', u'A', u'gold', u'dollar', u'had', u'been', u'propos', u'sever', u'time', u'in', u'the', u'1830', u'and', u'1840', u',', u'but', u'wa', u'not', u'initi', u'adopt', u'.', u'Congress', u'wa', u'final', u'galvan', u'into', u'action', u'by', u'the', u'increas', u'suppli', u'of', u'bullion', u'from', u'the', u'California', u'gold', u'rush', u',', u'and', u'in', u'1849', u'author', u'a', u'gold', u'dollar', u'.', u'In', u'it', u'earli', u'year', u',', u'silver', u'coin', u'were', u'be', u'hoard', u'or', u'export', u',', u'and', u'the', u'gold', u'dollar', u'found', u'a', u'readi', u'place', u'in', u'commerc', u'.', u'Silver', u'again', u'circul', u'after', u'Congress', u'requir', u'in', u'1853', u'that', u'new', u'coin', u'of', u'that', u'metal', u'be', u'made', u'lighter', u',', u'and', u'the', u'gold', u'dollar', u'becam', u'a', u'rariti', u'in', u'commerc', u'even', u'befor', u'feder', u'coin', u'vanish', u'from', u'circul', u'amid', u'the', u'econom', u'disrupt', u'of', u'the', u'American', u'Civil', u'War', u'.', u'Gold', u'did', u'not', u'circul', u'again', u'in', u'most', u'of', u'the', u'nation', u'until', u'1879', u',', u'and', u'even', u'then', u',', u'the', u'gold', u'dollar', u'did', u'not', u'regain', u'it', u'place', u'in', u'commerc', u'.', u'In', u'it', u'final', u'year', u',', u'struck', u'in', u'small', u'number', u',', u'it', u'wa', u'hoard', u'by', u'specul', u'and', u'mount', u'in', u'jewelri', u'.']\n"
     ]
    }
   ],
   "source": [
    "raw= \"\"\"\n",
    "The gold dollar was a coin struck as a regular issue by the United States Bureau of the Mint from 1849 to 1889. The coin had three types over its lifetime, all designed by Mint Chief Engraver James B. Longacre. The Type 1 issue had the smallest diameter of any United States coin ever minted. A gold dollar had been proposed several times in the 1830s and 1840s, but was not initially adopted. Congress was finally galvanized into action by the increased supply of bullion from the California gold rush, and in 1849 authorized a gold dollar. In its early years, silver coins were being hoarded or exported, and the gold dollar found a ready place in commerce. Silver again circulated after Congress required in 1853 that new coins of that metal be made lighter, and the gold dollar became a rarity in commerce even before federal coins vanished from circulation amid the economic disruption of the American Civil War. Gold did not circulate again in most of the nation until 1879, and even then, the gold dollar did not regain its place in commerce. In its final years, struck in small numbers, it was hoarded by speculators and mounted in jewelry.\n",
    "\"\"\"\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "print [porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'gold', 'doll', 'was', 'a', 'coin', 'struck', 'as', 'a', 'regul', 'issu', 'by', 'the', 'unit', 'stat', 'bureau', 'of', 'the', 'mint', 'from', '1849', 'to', '1889', '.', 'the', 'coin', 'had', 'three', 'typ', 'ov', 'it', 'lifetim', ',', 'al', 'design', 'by', 'mint', 'chief', 'engrav', 'jam', 'b.', 'longacr', '.', 'the', 'typ', '1', 'issu', 'had', 'the', 'smallest', 'diamet', 'of', 'any', 'unit', 'stat', 'coin', 'ev', 'mint', '.', 'a', 'gold', 'doll', 'had', 'been', 'propos', 'sev', 'tim', 'in', 'the', '1830s', 'and', '1840s', ',', 'but', 'was', 'not', 'init', 'adopt', '.', 'congress', 'was', 'fin', 'galv', 'into', 'act', 'by', 'the', 'increas', 'supply', 'of', 'bul', 'from', 'the', 'californ', 'gold', 'rush', ',', 'and', 'in', '1849', 'auth', 'a', 'gold', 'doll', '.', 'in', 'it', 'ear', 'year', ',', 'silv', 'coin', 'wer', 'being', 'hoard', 'or', 'export', ',', 'and', 'the', 'gold', 'doll', 'found', 'a', 'ready', 'plac', 'in', 'commerc', '.', 'silv', 'again', 'circ', 'aft', 'congress', 'requir', 'in', '1853', 'that', 'new', 'coin', 'of', 'that', 'met', 'be', 'mad', 'light', ',', 'and', 'the', 'gold', 'doll', 'becam', 'a', 'rar', 'in', 'commerc', 'ev', 'bef', 'fed', 'coin', 'van', 'from', 'circ', 'amid', 'the', 'econom', 'disrupt', 'of', 'the', 'am', 'civil', 'war', '.', 'gold', 'did', 'not', 'circ', 'again', 'in', 'most', 'of', 'the', 'nat', 'until', '1879', ',', 'and', 'ev', 'then', ',', 'the', 'gold', 'doll', 'did', 'not', 'regain', 'it', 'plac', 'in', 'commerc', '.', 'in', 'it', 'fin', 'year', ',', 'struck', 'in', 'smal', 'numb', ',', 'it', 'was', 'hoard', 'by', 'spec', 'and', 'mount', 'in', 'jewelry', '.']\n"
     ]
    }
   ],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "print [lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words are case sensitive in the Porter stemmer whereas in the Lancaster they are all in lowercase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words that characterize the branches\n",
    "\n",
    "*Exercises:* TF-IDF and the branches of philosophy.\n",
    "\n",
    "Setup. We want to start from a clean version of the philosopher pages with as little wiki-markup as possible. We needed it earlier to get the links, etc, but now we want a readable version. We can get a fairly nice version directly from the wikipedia API, simply call prop=extracts&exlimit=max&explaintext instead of prop=revisions as we did earlier. This will make the API return the text without links and other markup.\n",
    "\n",
    "* **Use this method to retrive a nice copy of all philosopher's text. You can, of course, also clean the existing pages using regular expressions, if you like (but that's probably more work).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wikipedia_root_api_url = \"http://en.wikipedia.org/w/api.php\"\n",
    "philosophers_dir = './philosophers'\n",
    "\n",
    "def download_wikipage(philosopher):\n",
    "    payload = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'prop': 'extracts',\n",
    "        'exlimit': 'max',\n",
    "        'explaintext': 'true',\n",
    "        'titles': philosopher\n",
    "    }\n",
    "    \n",
    "    response = requests.get(wikipedia_root_api_url, params=payload)\n",
    "    content = response.json()\n",
    "    \n",
    "    # If there are no pages associated to the philosopher, just skip it \n",
    "    if 'pages' not in content['query']:\n",
    "        return None\n",
    "        \n",
    "    philosopher_pages = content['query']['pages']\n",
    "    philosopher_content = philosopher_pages[philosopher_pages.keys()[0]]\n",
    "    \n",
    "    # If there's no content in the page, skip it as well \n",
    "    if 'extract' not in philosopher_content:\n",
    "        return None\n",
    "    \n",
    "    return content\n",
    "\n",
    "def save_to_file(file_name, json):\n",
    "    with io.open('./' + philosophers_dir + '/' + file_name + '.pickle', 'wb') as f:\n",
    "        pickle.dump(json, f)\n",
    "        \n",
    "def load_philosophers_from_file(file_name):\n",
    "    f = io.open(file_name, 'r', encoding='utf-8')\n",
    "\n",
    "    # Find all matches\n",
    "    philosophers_matches = re.findall(re_wiki_link, f.read())\n",
    "    return set(philosophers_matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# More advanced regex that captures links with whitespaces and doesn't require any manual pre-processing of the file\n",
    "re_wiki_link = r'\\*.*?\\[\\[([^\\[\\]|]+)[^\\[\\]]*\\]\\]' \n",
    "\n",
    "\n",
    "def create_philosophers_dict():\n",
    "    philosophers_branches = {}\n",
    "    \n",
    "    aestheticians_matches = load_philosophers_from_file('philosophers_aestheticians.txt')\n",
    "    epistemologists_matches = load_philosophers_from_file('philosophers_epistemologists.txt')\n",
    "    ethicists_matches = load_philosophers_from_file('philosophers_ethicists.txt')\n",
    "    logicians_matches = load_philosophers_from_file('philosophers_logicians.txt')\n",
    "    metaphysicians_matches = load_philosophers_from_file('philosophers_metaphysicians.txt')\n",
    "    sociopoliticians_matches = load_philosophers_from_file('philosophers_sociopolitical.txt')\n",
    "    \n",
    "    philosophers_unique = aestheticians_matches.union(epistemologists_matches) \\\n",
    "                                                .union(ethicists_matches) \\\n",
    "                                                .union(logicians_matches) \\\n",
    "                                                .union(logicians_matches) \\\n",
    "                                                .union(metaphysicians_matches) \\\n",
    "                                                .union(sociopoliticians_matches)\n",
    "                        \n",
    "    philosophers_unique = set(philosophers_unique)\n",
    "    \n",
    "    # Check if philosopher is in branch_name, and add the branch to his list of branches if so\n",
    "    def if_philosopher_in_branch(philosopher, content, branch_name, branch_matches):\n",
    "        if philosopher in branch_matches:\n",
    "            if branch_name in philosophers_branches:\n",
    "                philosophers_branches[branch_name][philosopher] = content\n",
    "            else:\n",
    "                # If the philosopher is not yet in the dictionary, create a new dict with the current branch\n",
    "                philosophers_branches[branch_name] = {philosopher: content}\n",
    "                \n",
    "    \n",
    "    # Helper method to check in each branch\n",
    "    def check_if_philosopher_in_one_branch(philosopher):\n",
    "        # Download content\n",
    "        content = download_wikipage(philosopher)\n",
    "        if not content:\n",
    "                return\n",
    "        philosopher_pages = content['query']['pages']\n",
    "        philosopher_content = philosopher_pages[philosopher_pages.keys()[0]]\n",
    "\n",
    "        philosopher_content = philosopher_content['extract']\n",
    "            \n",
    "        if_philosopher_in_branch(philosopher, philosopher_content, 'aestheticians', aestheticians_matches)\n",
    "        if_philosopher_in_branch(philosopher, philosopher_content, 'epistemologists', epistemologists_matches)\n",
    "        if_philosopher_in_branch(philosopher, philosopher_content, 'ethicists', ethicists_matches)\n",
    "        if_philosopher_in_branch(philosopher, philosopher_content, 'logicians', logicians_matches)\n",
    "        if_philosopher_in_branch(philosopher, philosopher_content, 'metaphysicians', metaphysicians_matches)\n",
    "        if_philosopher_in_branch(philosopher, philosopher_content, 'sociopoliticians', sociopoliticians_matches)\n",
    "    \n",
    "    \n",
    "    # For each philosopher, check in which branch they belong to\n",
    "    count = 0\n",
    "    print len(philosophers_unique)\n",
    "    for philosopher in philosophers_unique:\n",
    "        check_if_philosopher_in_one_branch(philosopher)\n",
    "        count += 1\n",
    "        print count\n",
    "        \n",
    "    return philosophers_branches\n",
    "\n",
    "# # Get all the files with the philosophers information\n",
    "# philosopher_files = get_list_of_philosophers_files(philosophers_dir)\n",
    "\n",
    "# philosophers_content = {}\n",
    "\n",
    "# for philosopher_file in philosopher_files:\n",
    "#     philosopher_wikipage = load_philosopher_from_file(philosopher_file)\n",
    "    \n",
    "#     philosopher_pages = philosopher_wikipage['query']['pages']\n",
    "#     philosopher_content = philosopher_pages[philosopher_pages.keys()[0]]\n",
    "#     philosopher_name = philosopher_content['title']\n",
    "    \n",
    "#     philosophers_content[philosopher_name] = philosopher_content\n",
    "    \n",
    "#     content = download_wikipage(philosopher_name)\n",
    "#     save_to_file(philosopher_name + '-extract', content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1013\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n"
     ]
    }
   ],
   "source": [
    "philosopher_branches = create_philosophers_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **First, check out the wikipedia page for TF-IDF. Explain in your own words the point of TF-IDF. **\n",
    "\n",
    "TF-IDF is a measure in information retrieval that captures how important a word is in a document compared to a a list of corpuses. A term will have a high TF-IDF if it appears frequently in a document but infrequently in other documents, whereas a term that appears frequently in all documents won't have a high TF-IDF.  \n",
    "* \n",
    "    * ** What does TF stand for?  ** TF stands for Term Frequency\n",
    "    * ** What does IDF stand for? **  IDF stands for Inverse Document Frequency \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Since we want to find out which words are important for each branch, so we're going to create six large documents, one per branch of philosophy. Tokenize the pages, and combine the tokens into one long list per branch. Remember the bullets below for success.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string \n",
    "philosopher_branches_tokens = {}\n",
    "\n",
    "# Collect english stopwords \n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def is_alphanum(input_string):\n",
    "    # TODO See if we need to do not all here instead (do we accept punctuation in words ?)\n",
    "    return all(char.isalnum() for char in input_string)\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Loop through each branch\n",
    "for branch, philosophers in philosopher_branches.iteritems():\n",
    "    #print len(philosophers)\n",
    "    tokens_current_branch = {}\n",
    "    # Collect for each branch the tokens\n",
    "    for philosopher, content in philosophers.iteritems():\n",
    "        tokens = nltk.word_tokenize(content)\n",
    "        philosopher_name = philosopher.lower()\n",
    "        \n",
    "        # Go through each token and check if we keep it\n",
    "        for token in tokens:\n",
    "            token_lower = token.lower()\n",
    "            # if token is a part of the philosopher name, not alphanum (like punctuation), is a stopword, \n",
    "            # or is a number, discard it\n",
    "            if len(token_lower) == 1 \\\n",
    "            or not is_alphanum(token_lower) \\\n",
    "            or is_number(token_lower) \\\n",
    "            or token_lower in philosopher_name \\\n",
    "            or token_lower in stopwords: \\\n",
    "                continue\n",
    "            # Add the token to the tokens of the current branch\n",
    "            if token_lower in tokens_current_branch:\n",
    "                tokens_current_branch[token_lower] += 1\n",
    "            else\n",
    "                tokens_current_branch[token_lower] = 1\n",
    "    \n",
    "    # Add the complete list of tokens to the branch \n",
    "    philosopher_branches_tokens[branch] = tokens_current_branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Now, we're ready to calculate the TF for each word. Use the method of your choice to find the top 5 terms within each branch. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "philosopher_branches_tokens_frequency = {}\n",
    "\n",
    "for branch, tokens in philosopher_branches_tokens.iteritems():\n",
    "    #freq_tokens = FreqDist(tokens)\n",
    "    freq_tokens = tokens\n",
    "    freq_tokens_sorted = sorted(freq_tokens.iteritems(), key=lambda (k,v): -v)\n",
    "    philosopher_branches_tokens_frequency[branch] = freq_tokens_sorted\n",
    "    print \"The top 5 terms for the branch %s are : %s\" % (branch, freq_tokens_sorted[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "    * **Describe similarities and differences between the branches.**\n",
    "    \n",
    "    A\n",
    "    * ** Why aren't the TFs not necessarily a good description of the branches? **\n",
    "    \n",
    "    A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Next, we calculate IDF for every word. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-bb94cb3f7ab6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtoken_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbranch1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mphilosopher_branches_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m                 \u001b[0mtoken_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mword_idf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtoken_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "N = len(philosopher_branches) # Number of branches\n",
    "\n",
    "word_idf = {}\n",
    "\n",
    "for branch, tokens in philosopher_branches_tokens.iteritems():\n",
    "    for token in tokens:\n",
    "        token_count = 0\n",
    "        for branch1, tokens1 in philosopher_branches_tokens.iteritems():\n",
    "            if token in tokens1:\n",
    "                token_count += 1\n",
    "        word_idf[token] = np.log(N / token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "    * ** What base logarithm did you use? Is that important? **\n",
    "    \n",
    "    A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **We're ready to calculate TF-IDF. Do that for each branch. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "philosophers_branches_idf = {[] for _ in xrange(6)}\n",
    "for branch, tfs in philosopher_branches_tokens_frequency.iteritems():\n",
    "    for word, tf:\n",
    "        philosophers_branches_idf[branch].append((word, tf /  word_idf[token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "    * **List the 10 top words for each branch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for branch, words_tf_idf in philosophers_branches_idf.iteritems():\n",
    "    words_tf_idf_sorted = sorted(words_tf_idf, key=lambda (w, tf_idf): -tf_idf)\n",
    "    print \"Top 10 words for branch %s : %s\" % (branch, words_tf_idf_sorted[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \n",
    "    * **Are these 10 words more descriptive of the branch? If yes, what is it about IDF that makes the words more informative?**\n",
    "    \n",
    "    A\n",
    "* **Normally, TF-IDF is used for single documents. What does TF-IDF tell us about the content of a single document in a collection.**\n",
    "\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
