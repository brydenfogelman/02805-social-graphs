{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7\n",
    "\n",
    "## Part 1: Processing real text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import cPickle as pickle\n",
    "import math\n",
    "import timeit\n",
    "from __future__ import division\n",
    "# custom module used for philosopher files\n",
    "import philosophers as ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: NLPP1e 3.12: 6, 30.\n",
    "\n",
    "**Q1:** Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "* [a-zA-Z]+\n",
    "\n",
    "Any character or set of character in the alapheblt regardless of the capitilization at least one time.\n",
    "ex. a, aASfdthsFJ, HELLO\n",
    "\n",
    "* [A-Z][a-z]*\n",
    "\n",
    "Matches to a captial letter at the start of the word and zero or more character after. Useful for matching to nouns or words that start at the beginning of a sentence.\n",
    "\n",
    "* p[aeiou]{,2}t\n",
    "\n",
    "Matches the first letter p and up to 2 vowels and the last letter t.\n",
    "\n",
    "* \\d+(\\.\\d+)?\n",
    "\n",
    "Matches integer and decimals numbers but can also match repeated decimal placeholders (ex. 3.4.5.1.). Will never match more then one '.' in a row but can match multiple digits between '.'. \n",
    "\n",
    "* ([^aeiou][aeiou][^aeiou])*\n",
    "\n",
    "Matches a non-vowel character then a vowel character followed by another non-vowel character zero or more times. So this will match all vowels that are \"sandwhiched\" by two consonants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{How} {aRe} {YOU} {A} {a}612{t}64{h} {lol} 3343 5.6.7 6..78.8 {this} {semseter} {pet} {pat} {pueit} {peit} {paat}\n",
      "{How} a{Re} {Y}{O}{U} {A} a612t64h lol 3343 5.6.7 6..78.8 this semseter pet pat pueit peit paat\n",
      "How aRe YOU A a612t64h lol 3343 5.6.7 6..78.8 this semseter {pet} {pat} pueit {peit} {paat}\n",
      "How aRe YOU A a{612t64}h lol {3343 5}.{6.7} {6}..{78.8} this semseter pet pat pueit peit paat\n",
      "{How aR}e{} {}Y{}O{}U{} {}A{ a6}1{}2{}t{}6{}4{}h{} {lol} {}3{}3{}4{}3{} {}5{}.{}6{}.{}7{} {}6{}.{}.{}7{}8{}.{}8{} {}t{his} {semset}e{}r{} {pet} {pat} {}p{}u{}e{}i{}t{} {}p{}e{}i{}t{} {}p{}a{}a{}t{}\n"
     ]
    }
   ],
   "source": [
    "# Test answers using nltk.re_show()\n",
    "test_string = 'How aRe YOU A a612t64h lol 3343 5.6.7 6..78.8 this semseter pet pat pueit peit paat'\n",
    "nltk.re_show('[a-zA-Z]+',test_string)\n",
    "nltk.re_show('[A-Z][a-z]*',test_string)\n",
    "nltk.re_show('p[aeiou]{,2}t',test_string)\n",
    "nltk.re_show('\\d+(.\\d+)?',test_string)\n",
    "nltk.re_show('([^aeiou][aeiou][^aeiou])*',test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and see if you observe any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "[u'The', u'gold', u'dollar', u'wa', u'a', u'coin', u'struck', u'as', u'a', u'regular', u'issu', u'by', u'the', u'Unit', u'State', u'Bureau', u'of', u'the', u'Mint', u'from', u'1849', u'to', u'1889', u'.', u'The', u'coin', u'had', u'three', u'type', u'over', u'it', u'lifetim', u',', u'all', u'design', u'by', u'Mint', u'Chief', u'Engrav', u'Jame', u'B.', u'Longacr', u'.', u'The', u'Type', u'1', u'issu', u'had', u'the', u'smallest', u'diamet', u'of', u'ani', u'Unit', u'State', u'coin', u'ever', u'mint', u'.', u'A', u'gold', u'dollar', u'had', u'been', u'propos', u'sever', u'time', u'in', u'the', u'1830', u'and', u'1840', u',', u'but', u'wa', u'not', u'initi', u'adopt', u'.', u'Congress', u'wa', u'final', u'galvan', u'into', u'action', u'by', u'the', u'increas', u'suppli', u'of', u'bullion', u'from', u'the', u'California', u'gold', u'rush', u',', u'and', u'in', u'1849', u'author', u'a', u'gold', u'dollar', u'.', u'In', u'it', u'earli', u'year', u',', u'silver', u'coin', u'were', u'be', u'hoard', u'or', u'export', u',', u'and', u'the', u'gold', u'dollar', u'found', u'a', u'readi', u'place', u'in', u'commerc', u'.', u'Silver', u'again', u'circul', u'after', u'Congress', u'requir', u'in', u'1853', u'that', u'new', u'coin', u'of', u'that', u'metal', u'be', u'made', u'lighter', u',', u'and', u'the', u'gold', u'dollar', u'becam', u'a', u'rariti', u'in', u'commerc', u'even', u'befor', u'feder', u'coin', u'vanish', u'from', u'circul', u'amid', u'the', u'econom', u'disrupt', u'of', u'the', u'American', u'Civil', u'War', u'.', u'Gold', u'did', u'not', u'circul', u'again', u'in', u'most', u'of', u'the', u'nation', u'until', u'1879', u',', u'and', u'even', u'then', u',', u'the', u'gold', u'dollar', u'did', u'not', u'regain', u'it', u'place', u'in', u'commerc', u'.', u'In', u'it', u'final', u'year', u',', u'struck', u'in', u'small', u'number', u',', u'it', u'wa', u'hoard', u'by', u'specul', u'and', u'mount', u'in', u'jewelri', u'.']\n",
      "\n",
      "Lancaster Stemmer\n",
      "['the', 'gold', 'doll', 'was', 'a', 'coin', 'struck', 'as', 'a', 'regul', 'issu', 'by', 'the', 'unit', 'stat', 'bureau', 'of', 'the', 'mint', 'from', '1849', 'to', '1889', '.', 'the', 'coin', 'had', 'three', 'typ', 'ov', 'it', 'lifetim', ',', 'al', 'design', 'by', 'mint', 'chief', 'engrav', 'jam', 'b.', 'longacr', '.', 'the', 'typ', '1', 'issu', 'had', 'the', 'smallest', 'diamet', 'of', 'any', 'unit', 'stat', 'coin', 'ev', 'mint', '.', 'a', 'gold', 'doll', 'had', 'been', 'propos', 'sev', 'tim', 'in', 'the', '1830s', 'and', '1840s', ',', 'but', 'was', 'not', 'init', 'adopt', '.', 'congress', 'was', 'fin', 'galv', 'into', 'act', 'by', 'the', 'increas', 'supply', 'of', 'bul', 'from', 'the', 'californ', 'gold', 'rush', ',', 'and', 'in', '1849', 'auth', 'a', 'gold', 'doll', '.', 'in', 'it', 'ear', 'year', ',', 'silv', 'coin', 'wer', 'being', 'hoard', 'or', 'export', ',', 'and', 'the', 'gold', 'doll', 'found', 'a', 'ready', 'plac', 'in', 'commerc', '.', 'silv', 'again', 'circ', 'aft', 'congress', 'requir', 'in', '1853', 'that', 'new', 'coin', 'of', 'that', 'met', 'be', 'mad', 'light', ',', 'and', 'the', 'gold', 'doll', 'becam', 'a', 'rar', 'in', 'commerc', 'ev', 'bef', 'fed', 'coin', 'van', 'from', 'circ', 'amid', 'the', 'econom', 'disrupt', 'of', 'the', 'am', 'civil', 'war', '.', 'gold', 'did', 'not', 'circ', 'again', 'in', 'most', 'of', 'the', 'nat', 'until', '1879', ',', 'and', 'ev', 'then', ',', 'the', 'gold', 'doll', 'did', 'not', 'regain', 'it', 'plac', 'in', 'commerc', '.', 'in', 'it', 'fin', 'year', ',', 'struck', 'in', 'smal', 'numb', ',', 'it', 'was', 'hoard', 'by', 'spec', 'and', 'mount', 'in', 'jewelry', '.']\n"
     ]
    }
   ],
   "source": [
    "raw = \"\"\"The gold dollar was a coin struck as a regular issue by the United States Bureau of the Mint from 1849 to \n",
    "        1889. The coin had three types over its lifetime, all designed by Mint Chief Engraver James B. Longacre. \n",
    "        The Type 1 issue had the smallest diameter of any United States coin ever minted. A gold dollar had been \n",
    "        proposed several times in the 1830s and 1840s, but was not initially adopted. Congress was finally galvanized \n",
    "        into action by the increased supply of bullion from the California gold rush, and in 1849 authorized a gold dollar. \n",
    "        In its early years, silver coins were being hoarded or exported, and the gold dollar found a ready place in commerce. \n",
    "        Silver again circulated after Congress required in 1853 that new coins of that metal be made lighter, and the \n",
    "        gold dollar became a rarity in commerce even before federal coins vanished from circulation amid the economic \n",
    "        disruption of the American Civil War. Gold did not circulate again in most of the nation until 1879, and even \n",
    "        then, the gold dollar did not regain its place in commerce. In its final years, struck in small numbers, it was \n",
    "        hoarded by speculators and mounted in jewelry.\"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "print \"Porter Stemmer\"\n",
    "print [porter.stem(t) for t in tokens]\n",
    "\n",
    "print \"\\nLancaster Stemmer\"\n",
    "print [lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notable differences:\n",
    "\n",
    "> * Lancaster stemmer removed the ar from dollar\n",
    "> * Lancaster transforms rarity into rar so it can match rare and rarity while the stemmer porter does not do the same. Many other cases of this example, Lancaster is going to the base word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Words that characterize the branches\n",
    "\n",
    "**Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only needs to be ran once\n",
    "# # create files that will not have any wikipedia markup\n",
    "\n",
    "# # set the directory to be seperate from the previous one used\n",
    "directory = './phil-no-markup'\n",
    "\n",
    "# # pass in custom payload\n",
    "# payload = {\n",
    "#     'action': 'query',\n",
    "#     'format': 'json',\n",
    "#     'prop': 'extracts',\n",
    "#     'exlimit': 'max',\n",
    "#     'explaintext': '',\n",
    "#     'rvprop': 'content'\n",
    "# }\n",
    "\n",
    "# # create the file dump\n",
    "# ph.file_dump(directory=directory, **payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the branches are aestheticians, epistemologists, ethicists, logicians, metaphysicians, social_politicals\n",
    "# convert the returned tuple to a list for iteration\n",
    "branches = list(ph.lists())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Explain in your own words the point of TF-IDF.** \n",
    "\n",
    "TF stands for term frequency and IDF stands for inverse document frequency. TF-IDF is a measure of how frequently a word appears in a document but is proportional to the amount of times it appears in a colllection of documents (corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phil_tokenizer(raw):\n",
    "    # create stemmer\n",
    "#     porter = nltk.PorterStemmer()\n",
    "    # impossible to use stem\n",
    "    \n",
    "    # get tokens from raw text\n",
    "    tokens = nltk.word_tokenize(raw)\n",
    "    \n",
    "    # retrieve common stop words form nltk\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    # remove some custom stop words, these words appear often but have no special meaning in the text\n",
    "    # ex. philosophy is expected to be used a lot, many philosphers attended university so this is a common term\n",
    "#     custom_stopwords = set(['also', 'one', 'philosophy', 'philosophical', 'philosopher', 'university', 'new', 'work', 'works'])\n",
    "    \n",
    "    # retrieve list of unique philosphers from custom module\n",
    "    unique_philosphers = set(ph.combined())\n",
    "    \n",
    "    # combined list of words to remove\n",
    "    removewords = stopwords.union(unique_philosphers) # .union(custom_stopwords)\n",
    "    \n",
    "    # isalpha returns true when no special characters and numbers are in a string\n",
    "    # create stemmed list of tokens not found in remove words or when isaplha is not satisfied\n",
    "    return [t.lower() for t in set(tokens) if t.lower() not in removewords and t.isalpha()]\n",
    "\n",
    "def open_file(directory, name):\n",
    "    try:\n",
    "        with open(directory + '/' + name + '.pickle', 'rb') as f:\n",
    "            try:\n",
    "                # files saved in tuple format\n",
    "                (phil, wikicontent) = pickle.load(f)\n",
    "                f.close()\n",
    "            except Exception as e:\n",
    "                print e, file\n",
    "        return wikicontent\n",
    "    except Exception as e:\n",
    "        print name + ',', \n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Philospher which content can not be retrieved due to exlimit issue: \n",
      "Arthur C. Danto, Friedrich Wilhelm Joseph von Schelling, Garry Hagberg, W.K. Wimsatt, Karl Wilhelm Friedrich von Schlegel, August Wilhelm von Schlegel, Yusuf Balasagun, Schopenhauer's aesthetics, George Birkhoff, George Edward Moore, G.W.F. Hegel, Friedrich A. Hayek, W.V.O. Quine, Tenzin Gyatso, Susan Wolf, William K. Frankena, Edward Hundert, Pierre AbÃ©lard, Seyyed Hossein Nasr, Francis Jeffrey, Friedrich Daniel Ernst Schleiermacher, Barron Lerner, Ismail Raji' al-Faruqi, David Friedrich Strauss, Karl Wilhelm Friedrich von Schlegel, Mortimer Adler, Hsun Tzu, Fazlur Rahman, Charles L. Stevenson, Mohandas Gandhi, Peter B. Andrews (mathematician), Luitzen Egbertus Jan Brouwer, John Duns Scotus, Peter of Spain (author), Pierre Abelard, William Craig (logician), Michael A. E. Dummett, Richard the Sophister, Adolf Fraenkel, David Kellogg Lewis, Carew Meredith, Richard Routley, Ahmed Raza Khan, Saunders MacLane, Frank Plumpton Ramsey, Henry Andrew Pogorzelski, Georg W. F. Hegel, David K. Lewis, Proclus Lycaeus, Willard V. O. Quine, Charles de Secondat, baron de Montesquieu, Umar bin al-Khattab, Maurice Brinton, Xun Zi, Mohandas Karamchand Gandhi, Mortimer Adler, Chankya, Bhimrao Ramji Ambedkar,\n"
     ]
    }
   ],
   "source": [
    "print 'Philospher which content can not be retrieved due to exlimit issue: '\n",
    "# combined list of tokens\n",
    "all_tokens = []\n",
    "for branch in branches:\n",
    "    # tokens for each branch\n",
    "    tokenized_branch = []\n",
    "    # for every philopsher in each branch tokenize and append to list\n",
    "    for phil in branch:\n",
    "        # get the wikicontent without mark up from file\n",
    "        wikicontent = open_file(directory, phil)\n",
    "        # get the tokens from the wikicontent\n",
    "        tokens = phil_tokenizer(wikicontent)\n",
    "        # add the tokens to the list of tokens\n",
    "        tokenized_branch.extend(tokens)\n",
    "    # save list of tokens in a branch to list of all tokens\n",
    "    all_tokens.append(tokenized_branch)\n",
    "\n",
    "# save branches to seperate lists by unpacking as a tuple\n",
    "aestheticians, epistemologists, ethicists, logicians, metaphysicians, social_politicals = tuple(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating TF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'philosophy', 152), (u'new', 145), (u'life', 136), (u'university', 135), (u'works', 133)]\n",
      "[(u'philosophy', 162), (u'philosophical', 113), (u'theory', 111), (u'university', 110), (u'new', 108)]\n",
      "[(u'philosophy', 347), (u'life', 314), (u'university', 294), (u'one', 293), (u'new', 293)]\n",
      "[(u'logic', 367), (u'university', 282), (u'philosophy', 263), (u'theory', 253), (u'mathematics', 251)]\n",
      "[(u'philosophy', 170), (u'philosophical', 115), (u'new', 114), (u'university', 103), (u'see', 102)]\n",
      "[(u'philosophy', 391), (u'political', 369), (u'new', 332), (u'life', 317), (u'university', 310)]\n"
     ]
    }
   ],
   "source": [
    "tf_lists = []\n",
    "for branch_tokens in all_tokens:\n",
    "    fd = nltk.FreqDist(branch_tokens)\n",
    "    tf_lists.append(fd.items())\n",
    "    print sorted(fd.items(), key=lambda x: -x[1])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24991\n",
      "46318\n",
      "85681\n",
      "112210\n",
      "133325\n",
      "174580\n",
      "Time to complete 0.313067913055\n"
     ]
    }
   ],
   "source": [
    "# create set of all unique words\n",
    "all_words = []\n",
    "\n",
    "for branch_tokens in all_tokens:\n",
    "    all_words.extend(set(branch_tokens))\n",
    "    print len(all_words)\n",
    "\n",
    "unique_words = set(all_words)\n",
    "\n",
    "# save results of idf to dict\n",
    "occurences = dict.fromkeys(unique_words, 0.)\n",
    "\n",
    "# number of branches\n",
    "N = len(all_tokens)\n",
    "\n",
    "# count = 0\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for word in all_words:\n",
    "    occurences[word] += 1\n",
    "    \n",
    "idf = {}\n",
    "\n",
    "for word, occurence in occurences.iteritems():\n",
    "    idf[word] = math.log(N/occurence,10)\n",
    "    \n",
    "stop = timeit.default_timer()\n",
    "print 'Time to complete %s' % (stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf['university']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'concerto', 1.4357653469175038, 1.8450980400142567, 0.7781512503836435), (u'violin', 1.3836706188572714, 1.7781512503836434, 0.7781512503836435), (u'aestheticians', 1.246644985441463, 1.6020599913279623, 0.7781512503836435), (u'ravenna', 1.246644985441463, 1.6020599913279623, 0.7781512503836435), (u'sibley', 1.246644985441463, 1.6020599913279623, 0.7781512503836435), (u'videogames', 1.1494237513283616, 1.4771212547196624, 0.7781512503836435), (u'semiology', 1.1494237513283616, 1.4771212547196624, 0.7781512503836435), (u'uncollected', 1.1494237513283616, 1.4771212547196624, 0.7781512503836435), (u'hogan', 1.1494237513283616, 1.4771212547196624, 0.7781512503836435), (u'droits', 1.1494237513283616, 1.4771212547196624, 0.7781512503836435)]\n",
      "\n",
      "\n",
      "[(u'complexit\\xe9', 1.246644985441463, 1.6020599913279623, 0.7781512503836435), (u'turri', 1.1494237513283616, 1.4771212547196624, 0.7781512503836435), (u'primatology', 1.1494237513283616, 1.4771212547196624, 0.7781512503836435), (u'glasersfeld', 1.1494237513283616, 1.4771212547196624, 0.7781512503836435), (u'infinitism', 1.0123981179125534, 1.3010299956639813, 0.7781512503836435), (u'antifragile', 1.0123981179125534, 1.3010299956639813, 0.7781512503836435), (u'tail', 1.0123981179125534, 1.3010299956639813, 0.7781512503836435), (u'bateson', 1.0123981179125534, 1.3010299956639813, 0.7781512503836435), (u'cassette', 1.0123981179125534, 1.3010299956639813, 0.7781512503836435), (u'descriptional', 1.0123981179125534, 1.3010299956639813, 0.7781512503836435)]\n",
      "\n",
      "\n",
      "[(u'bioethicist', 1.4357653469175038, 1.8450980400142567, 0.7781512503836435), (u'biomedical', 1.4357653469175038, 1.8450980400142567, 0.7781512503836435), (u'embryonic', 1.3220556332383773, 1.6989700043360187, 0.7781512503836435), (u'talmudic', 1.3220556332383773, 1.6989700043360187, 0.7781512503836435), (u'counseling', 1.3220556332383773, 1.6989700043360187, 0.7781512503836435), (u'hauerwas', 1.3220556332383773, 1.6989700043360187, 0.7781512503836435), (u'hons', 1.246644985441463, 1.6020599913279623, 0.7781512503836435), (u'queensland', 1.246644985441463, 1.6020599913279623, 0.7781512503836435), (u'neuroethics', 1.246644985441463, 1.6020599913279623, 0.7781512503836435), (u'interfaith', 1.246644985441463, 1.6020599913279623, 0.7781512503836435)]\n",
      "\n",
      "\n",
      "[(u'metamathematics', 1.6700122144464133, 2.1461280356782377, 0.7781512503836435), (u'axiomatization', 1.6179174863861812, 2.0791812460476247, 0.7781512503836435), (u'kleene', 1.5885122704798962, 2.041392685158225, 0.7781512503836435), (u'topology', 1.5885122704798962, 2.041392685158225, 0.7781512503836435), (u'smullyan', 1.5206962522730798, 1.9542425094393248, 0.7781512503836435), (u'feferman', 1.5206962522730798, 1.9542425094393248, 0.7781512503836435), (u'kazimierz', 1.4808918529703725, 1.9030899869919433, 0.7781512503836435), (u'countable', 1.4808918529703725, 1.9030899869919433, 0.7781512503836435), (u'lw\\xf3w', 1.4357653469175038, 1.8450980400142567, 0.7781512503836435), (u'kreisel', 1.4357653469175038, 1.8450980400142567, 0.7781512503836435)]\n",
      "\n",
      "\n",
      "[(u'graecia', 1.1494237513283616, 1.4771212547196624, 0.7781512503836435), (u'univocity', 1.1494237513283616, 1.4771212547196624, 0.7781512503836435), (u'geocentrism', 1.1494237513283616, 1.4771212547196624, 0.7781512503836435), (u'leucippus', 1.1494237513283616, 1.4771212547196624, 0.7781512503836435), (u'melchert', 1.1494237513283616, 1.4771212547196624, 0.7781512503836435), (u'vlastos', 1.1494237513283616, 1.4771212547196624, 0.7781512503836435), (u'heliocentrism', 1.1494237513283616, 1.4771212547196624, 0.7781512503836435), (u'militare', 1.0123981179125534, 1.3010299956639813, 0.7781512503836435), (u'maior', 1.0123981179125534, 1.3010299956639813, 0.7781512503836435), (u'distinctio', 1.0123981179125534, 1.3010299956639813, 0.7781512503836435)]\n",
      "\n",
      "\n",
      "[(u'samurai', 1.5206962522730798, 1.9542425094393248, 0.7781512503836435), (u'tokugawa', 1.5206962522730798, 1.9542425094393248, 0.7781512503836435), (u'edo', 1.4808918529703725, 1.9030899869919433, 0.7781512503836435), (u'shogunate', 1.4357653469175038, 1.8450980400142567, 0.7781512503836435), (u'reloaded', 1.4357653469175038, 1.8450980400142567, 0.7781512503836435), (u'bureaucrats', 1.3836706188572714, 1.7781512503836434, 0.7781512503836435), (u'debord', 1.3220556332383773, 1.6989700043360187, 0.7781512503836435), (u'neoconservatism', 1.3220556332383773, 1.6989700043360187, 0.7781512503836435), (u'mutualism', 1.3220556332383773, 1.6989700043360187, 0.7781512503836435), (u'razan', 1.3220556332383773, 1.6989700043360187, 0.7781512503836435)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for tf_branch in tf_lists:\n",
    "    tfidf_branch = []\n",
    "    for items in tf_branch:\n",
    "        word = items[0]\n",
    "        tf = items[1]\n",
    "        idf_value = idf[word]\n",
    "        tfidf = (1+math.log(tf,10)) * idf_value\n",
    "        tfidf_branch.append((word, tfidf, (1+math.log(tf,10)), idf_value))\n",
    "    results.append(tfidf_branch)\n",
    "    \n",
    "for branch in results:\n",
    "    print sorted(branch, key=lambda x: -x[1])[:10]\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sentinment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe briefly how the list was generated.\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\tword\thappiness_rank\thappiness_average\thappiness_standard_deviation\ttwitter_rank\tgoogle_rank\tnyt_rank\tlyrics_rank\n",
    "f = io.open('./labMIT-1.0.txt','r')\n",
    "\n",
    "word_happiness_index = {}\n",
    "\n",
    "for line in f:\n",
    "    cols = line.split()\n",
    "    \n",
    "    \n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
