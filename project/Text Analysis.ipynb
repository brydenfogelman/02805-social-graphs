{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "\n",
    "Below is some basic initilization, which involves connecting to our remote MongoDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Custom helper file\n",
    "import elections_helper as helper\n",
    "from elections_helper import display_table\n",
    "\n",
    "import numpy as np\n",
    "import re, string, operator, pickle, nltk, pprint, math\n",
    "\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globally Defined Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Twitter users really like these words but we just don't get it.\n",
    "other_stopwords = ['like','get']\n",
    "\n",
    "# retrieve common stop words form nltk\n",
    "stopwords = set(nltk.corpus.stopwords.words('english') + other_stopwords)\n",
    "\n",
    "# Retrieved dicationary containing word mapped with its happiness index\n",
    "words_happiness = pickle.load(open('./data_files/sentiment.pickle','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = helper.setup_mongo_client(properties_file='./properties/db.properties')\n",
    "\n",
    "tweet_collection, _ = helper.get_collections(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding our Data\n",
    "\n",
    "Initially, let's display some of the content in the tweets, practice making queries and looking and commonly used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet #1: RT @Ethan_Booker: TRUMP: look at this Hillary mask. it's hideous! just like her sou--\n",
      "AIDE: *whispers in ear*\n",
      "TRUMP: this mask is beautiful…\n",
      "Tweet #2: TRUMP: look at this Hillary mask. it's hideous! just like her sou--\n",
      "AIDE: *whispers in ear*\n",
      "TRUMP: this mask is bea… https://t.co/IpquTCKM9F\n",
      "Tweet #3: RT @FoxNews: #DonaldTrump's daughter-in-law, Lara Trump, blasted #HillaryClinton in a new interview. https://t.co/apANzmnyQU #Election2016…\n",
      "Tweet #4: #DonaldTrump's daughter-in-law, Lara Trump, blasted #HillaryClinton in a new interview. https://t.co/apANzmnyQU… https://t.co/eMohElI9aR\n",
      "Tweet #5: WikiLeaks: DNC And CNN Colluded On Questions For Trump, Cruz https://t.co/VDETfDgyLi\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "cur = tweet_collection.find({}, projection={'text': True}, limit = N)\n",
    "\n",
    "for i in xrange(N):\n",
    "    tweet_text = cur.next()\n",
    "    print \"Tweet #%s: %s\" % (i+1, tweet_text['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note:\n",
    "* Excess text for a retweeted message. ex. RT @MassDeception1:\n",
    "* Links in tweets\n",
    "* Mentions in tweets\n",
    "\n",
    "First off, we don't need to include retweeted message since they will have the same sentiment as the original. We can use regular expressions to filter out links in the format https://t.co/... and mentions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_hashtags = set([])\n",
    "\n",
    "def tweet_tokenizer(text, extra_stopwords=set([])):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # matches mentions, ampersands and twitter links\n",
    "    remove_pattern = r\"(?:@[\\w]*|https://t.co/[\\w]*|&[\\w]*)\" \n",
    "   \n",
    "    # matches text found after hashtag\n",
    "    hashtag_pattern = r\"#([^\\s]*)\"\n",
    "    \n",
    "    # pattern to remove puncuation except punctuation x\n",
    "    punc_pattern = lambda x:  r\"[^\\w\\d\" + x + \"\\s]+\"\n",
    "    \n",
    "    # match words from hashtag\n",
    "    hashtag_word_pattern = r'([A-Z][^A-Z]*|[a-z][a-z]*)'\n",
    "    \n",
    "    # create a list of all hashtags (not including hash symbol)\n",
    "    hashtags = re.findall(hashtag_pattern, text)\n",
    "    \n",
    "    # add hashtags to global set\n",
    "    all_hashtags.update(hashtags)\n",
    "    \n",
    "    # remove punctuation from hashtags in case it exists\n",
    "    hashtags = [re.sub(punc_pattern(''),' ', s) for s in hashtags]\n",
    "    \n",
    "    # split hashtag into words\n",
    "    hashtag_tokens = re.findall(hashtag_word_pattern, ' '.join(hashtags))\n",
    "\n",
    "    # remove mentions and links from tweet\n",
    "    text = re.sub(remove_pattern,'', text)\n",
    "    \n",
    "    # replaces punctuation with a space, removes hashtags\n",
    "    text = re.sub(punc_pattern(\"'#\"),' ', text)\n",
    "    \n",
    "    # create a combined string of tweet text and words from hashtag\n",
    "    text = text + ' '.join(hashtag_tokens)\n",
    "    \n",
    "    # split text at whitespace\n",
    "    tokens = set(text.split())\n",
    "    \n",
    "    # try stemming\n",
    "    # stemmer = SnowballStemmer(\"english\")\n",
    "    # stemmer.stem(t.lower())\n",
    "    \n",
    "    # remove if not in the alphabet and not in stopwords, set to lowercase\n",
    "    return [t.lower() for t in tokens \\\n",
    "                if t.lower() not in stopwords.union(extra_stopwords) and t.isalpha() and len(t) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet #1: [u'ear', u'trump', u'bea', u'mask', u'like', u'whispers', u'hillary', u'sou', u'aide', u'hideous', u'look']\n",
      "TRUMP: look at this Hillary mask. it's hideous! just like her sou--\n",
      "AIDE: *whispers in ear*\n",
      "TRUMP: this mask is bea… https://t.co/IpquTCKM9F\n",
      "Tweet #2: [u'daughter', u'trump', u'hillary', u'donald', u'lara', u'clinton', u'interview', u'new', u'law', u'blasted']\n",
      "#DonaldTrump's daughter-in-law, Lara Trump, blasted #HillaryClinton in a new interview. https://t.co/apANzmnyQU… https://t.co/eMohElI9aR\n",
      "Tweet #3: [u'dnc', u'cruz', u'questions', u'cnn', u'wikileaks', u'colluded', u'trump']\n",
      "WikiLeaks: DNC And CNN Colluded On Questions For Trump, Cruz https://t.co/VDETfDgyLi\n",
      "Tweet #4: [u'never', u'clinton', u'believed', u'emails', u'person', u'hillary', u'johnpodesta', u'wikileaks', u'podesta']\n",
      "Hillary Clinton is not a person who can be believed #NeverHillary #wikileaks #johnpodesta #PodestaEmails… https://t.co/ity0m0nJLL\n",
      "Tweet #5: [u'obama', u'clinton', u'annihilate', u'would', u'romney', u'hillary', u'clobber', u'bloomberg', u'poll', u'trump']\n",
      "Bloomberg Poll:\n",
      "Obama would annihilate Trump 53-41. \n",
      "Romney would clobber Hillary Clinton 50-40.\n",
      "https://t.co/7TD7WIO2vR\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "cur = tweet_collection.find({'retweeted': False}, projection={'text': True}, limit = N)\n",
    "\n",
    "for i in xrange(N):\n",
    "    tweet = cur.next()\n",
    "    text = tweet['text']\n",
    "    tokens = tweet_tokenizer(text)\n",
    "    \n",
    "    print \"Tweet #%s: %s\" % (i+1, tokens)\n",
    "    print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_generator(query, get_string=None):\n",
    "    \n",
    "    # if query is a list we need to aggregate\n",
    "    if type(query) is list:\n",
    "        cur = tweet_collection.aggregate(query)\n",
    "    else:\n",
    "        cur = tweet_collection.find(query)\n",
    "\n",
    "    for document in cur:\n",
    "        _id = document['_id']\n",
    "        if get_string:\n",
    "            result = get_string(document)\n",
    "            if result:\n",
    "                yield (result, _id)\n",
    "        # otherwise yield document\n",
    "        else:\n",
    "            yield (document, _id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dicts(*dict_args):\n",
    "    '''\n",
    "    Given any number of dicts, shallow copy and merge into a new dict,\n",
    "    precedence goes to key value pairs in latter dicts.\n",
    "    '''\n",
    "    result = {}\n",
    "    for dictionary in dict_args:\n",
    "        result.update(dictionary)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dict_representation(X, feature_names, merge=False):\n",
    "    '''\n",
    "    Convert sparse matrix representation to dictionary.\n",
    "    '''\n",
    "    dict_vectorizer = DictVectorizer()\n",
    "\n",
    "    # set feature names so dictionaries can be unpacked\n",
    "    dict_vectorizer.feature_names_ = feature_names\n",
    "\n",
    "    # merge dictionaries\n",
    "    if merge:\n",
    "        return merge_dicts(*dict_vectorizer.inverse_transform(X))\n",
    "    # keep seperate\n",
    "    else:\n",
    "        return dict_vectorizer.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def display_table(data, title=None, limit=20, **kwargs):\n",
    "#     if title:            \n",
    "#         print title + ' (limited to %s results)' % limit\n",
    "#     if type(data[0]) == tuple:\n",
    "#         data = [[str(tup[0]), str(tup[1])] for tup in data[:limit]]\n",
    "#     print tabulate(data[:limit], tablefmt=\"fancy_grid\", **kwargs)\n",
    "#     print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_dict(d, reverse=True):\n",
    "    return sorted(d.items(), key=operator.itemgetter(1), reverse=reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Frequency Distribution and TF-IDF\n",
    "\n",
    "For TF-IDF, we will combine all the tweets from the same location and same query since the document size for each individual tweet is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bag_words(query, get_string=None, extra_stopwords=set([])):\n",
    "    # Create a count vectorizer to convert tweets to bag of word representation\n",
    "    # count_vectorizer = CountVectorizer(tokenizer=tokenizer)\n",
    "    \n",
    "    # gen = tweet_generator(query, get_string)\n",
    "\n",
    "    # Create a matrix passing in a generator containing tweets\n",
    "    # bag_of_words = count_vectorizer.fit_transform(gen)\n",
    "    bag_words = {}\n",
    "    words = []\n",
    "    count = 0\n",
    "    \n",
    "    for text, _id in tweet_generator(query, get_string):\n",
    "        \n",
    "        tokens = tweet_tokenizer(text, extra_stopwords)\n",
    "        \n",
    "        bag_words[_id] = tokens\n",
    "        \n",
    "        words.extend(tokens)\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    print \"Total number of items retrieved is %s\" % count\n",
    "\n",
    "    # Save the indices of\n",
    "    # feature_names = count_vectorizer.get_feature_names()\n",
    "\n",
    "    # Get a dictionary representation of the bag of words matrix\n",
    "    # This can be used to create a tweet id to token dictionary\n",
    "    # bag_of_words_dictionary = get_dict_representation(bag_of_words, feature_names)\n",
    "    \n",
    "    # return bag_of_words, bag_of_words_dictionary, feature_names\n",
    "    return bag_words, set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_term_frequency(bag_words):\n",
    "    \n",
    "    tf = {}\n",
    "    \n",
    "#     def word_count(inner):\n",
    "#         # For each word from the list of tokens\n",
    "#         for k in inner.keys():\n",
    "#             # if the key does not exist, set to 1 \n",
    "#             if freq_dist.get(k,0) == 0:\n",
    "#                 freq_dist[k] = 1\n",
    "#             # add one if the key exists\n",
    "#             else:\n",
    "#                 freq_dist[k] += 1\n",
    "    \n",
    "#     if type(bag_of_words) is list:\n",
    "#         for inner in bag_of_words:\n",
    "#             word_count(inner)\n",
    "#     else:\n",
    "#         word_count(X)\n",
    "\n",
    "    for _, tokens in bag_words.iteritems():\n",
    "        for word in tokens:\n",
    "            # if the key does not exist, set to 1 \n",
    "            if tf.get(word,0) == 0:\n",
    "                tf[word] = 1\n",
    "            # add one if the key exists\n",
    "            else:\n",
    "                tf[word] += 1\n",
    "        \n",
    "    return tf, sort_dict(tf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tfidf(bag_words, tf, normalize=True, min_frequency=1):\n",
    "    \n",
    "    idf = {}\n",
    "    tfidf = {}\n",
    "    \n",
    "    for _id, tokens in bag_words.iteritems():\n",
    "        \n",
    "        tfidf[_id] = {}\n",
    "        \n",
    "        for word in set(tokens):\n",
    "            \n",
    "            tfidf[_id][word] = None\n",
    "            \n",
    "            # if the key does not exist, set to 1 \n",
    "            if idf.get(word,0) == 0:\n",
    "                idf[word] = 1\n",
    "            # add one if the key exists\n",
    "            else:\n",
    "                idf[word] += 1\n",
    "                \n",
    "    N = len(bag_words)\n",
    "    print \"Size of corpus (number of documents) %s\" % N\n",
    "    \n",
    "    for _id, inner in tfidf.iteritems():\n",
    "        for word in inner.keys():\n",
    "            frequency = tf[word]\n",
    "            if frequency <= min_frequency:\n",
    "                continue\n",
    "            if normalize:\n",
    "                frequency = (1 + math.log(frequency,10))\n",
    "\n",
    "            tfidf[_id][word] = math.log(1 + N / idf[word],10) * frequency\n",
    "    \n",
    "    # Create a transformer to convert to a tf-idf representation\n",
    "    # tfidf_transformer = TfidfTransformer(sublinear_tf=True)\n",
    "\n",
    "    # Calculate tf-idf using bag of words matrix\n",
    "    # tfidf = tfidf_transformer.fit_transform(bag_of_words)\n",
    "\n",
    "    # Create a dictionary representation of the sparse matrix\n",
    "    # tfidf_dictionary = get_dict_representation(tfidf, feature_names, merge=True)\n",
    "    \n",
    "    #if sort:\n",
    "    #    tfidf_sorted = sort_dict(tfidf_dictionary)\n",
    "    \n",
    "    #return tfidf, tfidf_dictionary, tfidf_sorted\n",
    "    return tfidf, sort_dict(merge_dicts(*[v for k,v in tfidf.iteritems()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Tweets\n",
    "\n",
    "First, all the tweets will be used to calculate the frequency distribution and TF-IDF (not including retweets). \n",
    "\n",
    "1. Calculating the frequency distribution.\n",
    "2. Calculating the TF-DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = {'retweeted': False}\n",
    "get_string = lambda x: x['text']\n",
    "# Find a bag of words representation\n",
    "bag_words, unique_words = get_bag_words(query, get_string)\n",
    "\n",
    "# Find the frequency distribution\n",
    "tf, tf_sorted = get_term_frequency(bag_words)\n",
    "print \"Total number of unique words is %s\\n\" % len(unique_words)\n",
    "\n",
    "print tf_sorted[:10]\n",
    "# display_table(tf_sorted, title=\"Most frequent words from Tweets\", headers=['Words','Frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the frequent words contain the words we used in our queries when getting Twitter data. Let's take a look at what are the most popular words when we ignore these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_stopwords = [u'financial', u'mexico', u'narrates', u'new', u'final', u'market', u'trump ', \\\n",
    "                   u'riot', u'cabinet', u'hillary', u'president', u'america', u'day', u'thoughts', \\\n",
    "                   u'stock', u'clinton ', u'positions', u'weed', u'planet', u'electionnight', u'clinton', \\\n",
    "                   u'obama', u'elections2016 ', u'elect', u'trump', u'canadian', u'donald', u'election', \\\n",
    "                   u'earth', u'still','night']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find a bag of words representation\n",
    "bag_words, unique_words = get_bag_words(query, get_string, query_stopwords)\n",
    "\n",
    "# Find the frequency distribution\n",
    "_, tf_sorted = get_term_frequency(bag_words)\n",
    "print tf_sorted[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will calculate the TF-IDF in three ways:\n",
    "\n",
    "1. Smooth IDF and TF \n",
    "> ( 1 + log(size/occurences) ) * tf\n",
    "2. Smooth IDF and normalized TF\n",
    "> ( 1 + log(size/occurences) ) * (1 + log(tf))\n",
    "3. Smooth IDF and normalized TF with a minimum TF of 10 \n",
    "> ( 1 + log(size/occurences) ) * (1 + log(tf)) if tf >= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf, tfidf_sorted = get_tfidf(bag_words, tf, normalize=False)\n",
    "print tfidf_sorted[:5]\n",
    "\n",
    "_, tfidf_sorted = get_tfidf(bag_words, tf)\n",
    "print tfidf_sorted[:5]\n",
    "\n",
    "_, tfidf_sorted = get_tfidf(bag_words, tf, min_frequency=10)\n",
    "print tfidf_sorted[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places\n",
    "\n",
    "Next, tweets that have a location of origin will be used and retweets will not be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items retrieved is [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Total number of unique words is 0\n",
      "\n",
      "Most frequent words from Tweets (limited to 10 results)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-358-fb4effadf863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Total number of unique words is %s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdisplay_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_sorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Most frequent words from Tweets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Words'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Frequency'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/brydenfogelman/DTU/social-graphs/02805-social-graphs/project/elections_helper.py\u001b[0m in \u001b[0;36mdisplay_table\u001b[0;34m(data, title, limit, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' (limited to %s results)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mtabulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtablefmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fancy_grid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "query = [\n",
    "    {\n",
    "        '$match': {'retweeted': False, 'place': {'$ne': None}}\n",
    "    },\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$place.country\",\n",
    "            'text': {'$push': '$text'}\n",
    "        }\n",
    "    },\n",
    "        {\n",
    "        \"$project\": {\n",
    "            \"location\": \"$_id.location\",\n",
    "            'all_text': '$text'\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "get_string = lambda x: ' '.join(x['all_text']) if len(x['all_text']) > 1000 else None\n",
    "\n",
    "# Find a bag of words representation\n",
    "bag_words, unique_words = get_bag_words(query, get_string, query_stopwords)\n",
    "\n",
    "# Find the frequency distribution\n",
    "tf, tf_sorted = get_term_frequency(bag_words)\n",
    "print \"Total number of unique words is %s\\n\" % len(unique_words)\n",
    "\n",
    "display_table(tf_sorted, title=\"Most frequent words from Tweets\", limit=10, headers=['Words','Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of corpus (number of documents) 12\n",
      "Canada\n",
      "[(u'dinah', 1.353897758656518), (u'groping', 1.353897758656518), (u'sanctuary', 1.353897758656518)]\n",
      "\n",
      "United Kingdom\n",
      "[(u'weekly', 1.353897758656518), (u'brazilian', 1.353897758656518), (u'brexiteers', 1.353897758656518)]\n",
      "\n",
      "Australia\n",
      "[(u'barking', 1.6454294022461016), (u'aap', 1.353897758656518), (u'presidentelect', 1.2483122772269863)]\n",
      "\n",
      "South Africa\n",
      "[(u'malema', 1.6454294022461016), (u'scoop', 1.353897758656518), (u'wud', 1.353897758656518)]\n",
      "\n",
      "United Arab Emirates\n",
      "[(u'modiji', 1.353897758656518), (u'surgical', 1.353897758656518), (u'evasion', 1.353897758656518)]\n",
      "\n",
      "Mexico\n",
      "[(u'puto', 1.6454294022461016), (u'jihadis', 1.353897758656518), (u'vive', 1.353897758656518)]\n",
      "\n",
      "India\n",
      "[(u'hindus', 1.6454294022461016), (u'jai', 1.6454294022461016), (u'din', 1.6454294022461016)]\n",
      "\n",
      "France\n",
      "[(u'vive', 1.353897758656518), (u'brexiteers', 1.353897758656518), (u'demonstrators', 1.353897758656518)]\n",
      "\n",
      "United States\n",
      "[(u'tantrums', 1.6454294022461016), (u'damns', 1.6454294022461016), (u'balloon', 1.6454294022461016)]\n",
      "\n",
      "Sweden\n",
      "[(u'swings', 1.2483122772269863), (u'hood', 1.2483122772269863), (u'presumably', 1.2483122772269863)]\n",
      "\n",
      "Germany\n",
      "[(u'rewarded', 1.2483122772269863), (u'bowls', 1.2483122772269863), (u'normalizing', 1.2483122772269863)]\n",
      "\n",
      "Brazil\n",
      "[(u'foda', 1.6454294022461016), (u'fora', 1.6454294022461016), (u'muito', 1.6454294022461016)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf, tfidf_sorted = get_tfidf(bag_words, tf, normalize=True, min_frequency=2)\n",
    "\n",
    "for place, values in tfidf.iteritems():\n",
    "    print place\n",
    "    print sort_dict(values)[:3]\n",
    "    print\n",
    "# display_table(tfidf_sorted, title=\"Highest TF-IDF words from Tweets\", limit=15, headers=['Words','TF-IDF'])\n",
    "\n",
    "# tfidf_sorted.reverse()\n",
    "# display_table(tfidf_sorted, title=\"Lowest TF-IDF words from Tweets\", limit=15, headers=['Words','Frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = [{'$match': {'retweeted': False}}, \n",
    "         {\"$group\": { \"_id\": \"$root_query\", 'text': {'$push': '$text'}}},\n",
    "         {\"$project\": {'all_text': '$text'}}]\n",
    "\n",
    "get_string = lambda x: ' '.join(x['all_text'])\n",
    "\n",
    "# Find a bag of words representation\n",
    "bag_words, unique_words = get_bag_words(query, get_string)\n",
    "\n",
    "# Find the frequency distribution\n",
    "tf, tf_sorted = get_term_frequency(bag_words)\n",
    "\n",
    "print \"Total number of unique words is %s\\n\" % len(unique_words)\n",
    "\n",
    "display_table(tf_sorted, title=\"Most frequent words from Tweets\", limit=10, headers=['Words','Frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment(tokens):\n",
    "    \n",
    "    combined_happiness = 0\n",
    "    words_with_no_sentiment = set()\n",
    "    words_sentiment_count = 0\n",
    "    \n",
    "    # Go through each token and if we have a sentiment for it, add it to the combined happiness score\n",
    "    for token in tokens:\n",
    "        # get the happiness value, otherwise return zero\n",
    "        happiest_value = words_happiness.get(token,0)\n",
    "        \n",
    "        # if a happiness value exists keep track \n",
    "        if happiest_value != 0:\n",
    "            combined_happiness += happiest_value\n",
    "            words_sentiment_count += 1\n",
    "        # save the words that have no happiness index\n",
    "        else:\n",
    "            words_with_no_sentiment.add(token)\n",
    "    \n",
    "    # Safe check to avoid division by 0\n",
    "    if combined_happiness == 0: \n",
    "        avg_sentiment_score = 0 \n",
    "    else: \n",
    "        avg_sentiment_score = combined_happiness / words_sentiment_count\n",
    "    \n",
    "    return avg_sentiment_score, words_with_no_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_sentiment = {}\n",
    "tweets_with_no_sentiment = []\n",
    "all_words_with_no_sentiment = set()\n",
    "\n",
    "for tweet in tweet_iterable(text_only=False):\n",
    "    text = tweet['text']\n",
    "    _id = tweet['_id']\n",
    "    \n",
    "    tokens = tweet_tokenizer(text)\n",
    "    \n",
    "    avg_sentiment_score, words_with_no_sentiment = get_sentiment(tokens)\n",
    "    \n",
    "    # update set of words with no sentiment\n",
    "    if len(words_with_no_sentiment) > 0:\n",
    "        all_words_with_no_sentiment = all_words_with_no_sentiment.union(words_with_no_sentiment)\n",
    "    \n",
    "    # If we didn't find any sentiment for the tweet, save the tweet id\n",
    "    if avg_sentiment_score == 0:\n",
    "        tweets_with_no_sentiment.append(_id)\n",
    "    # otherwise save the sentiment to the dictionary\n",
    "    else:\n",
    "        tweet_sentiment[_id] = avg_sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71265\n",
      "71370\n",
      "105\n",
      "26128\n"
     ]
    }
   ],
   "source": [
    "print len(tweet_sentiment)\n",
    "cur = tweet_collection.find({'retweeted': False})\n",
    "print cur.count()\n",
    "print len(tweets_with_no_sentiment)\n",
    "print len(all_words_with_no_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average sentiment of tweet sentiment is   5.57\n",
      "The stanard deviation of tweet sentiment is   0.47\n",
      "\n",
      "Percentage of tweets that have a sentiment    99.85%\n",
      "Percentage of words that have a sentiment     29.33%\n",
      "\n",
      "The happiest tweet is                         8.18\n",
      "Tweet text: @IcomOfficiel @UNESCO Excellent.\n",
      "BUT coinciding with https://t.co/2UoluNULzY\n",
      "\n",
      "The saddest tweet is                          1.56\n",
      "Tweet text: @xtremevicky \"Harambe died for this?\"\n",
      "#Elections2016 😂 https://t.co/ygtf0tnjl4\n"
     ]
    }
   ],
   "source": [
    "values = tweet_sentiment.values()\n",
    "\n",
    "average_sentiment = np.average(values)\n",
    "std_sentiment = np.std(values)\n",
    "happiest_value = max(values)\n",
    "saddest_value = min(values)\n",
    "\n",
    "tweet_percent = (len(tweet_sentiment) / (len(tweets_with_no_sentiment) + len(tweet_sentiment))) * 100\n",
    "word_percent = (1 - len(all_words_with_no_sentiment) / len(unique_words)) * 100\n",
    "\n",
    "happiest_tweet_id = tweet_sentiment.keys()[values.index(happiest_value)]\n",
    "saddest_tweet_id = tweet_sentiment.keys()[values.index(saddest_value)]\n",
    "\n",
    "happy_tweet = tweet_collection.find_one({'_id': happiest_tweet_id})['text']\n",
    "sad_tweet = tweet_collection.find_one({'_id': saddest_tweet_id})['text']\n",
    "\n",
    "print \"The average sentiment of tweet sentiment is   %2.2f\" % average_sentiment\n",
    "print \"The stanard deviation of tweet sentiment is   %2.2f\" % std_sentiment\n",
    "print ''\n",
    "print \"Percentage of tweets that have a sentiment    %2.2f%%\" % tweet_percent\n",
    "print \"Percentage of words that have a sentiment     %2.2f%%\" % word_percent\n",
    "print ''\n",
    "print \"The happiest tweet is                         %2.2f\" % happiest_value\n",
    "print \"Tweet text: %s\" % happy_tweet\n",
    "print ''\n",
    "print \"The saddest tweet is                          %2.2f\" % saddest_value\n",
    "print \"Tweet text: %s\" % sad_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for w in tweet_tokenizer(happy_tweet):\n",
    "    print words_happiness.get(w,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['Tweet ID','Happiness Index']\n",
    "\n",
    "display_table(sort_dict(tweet_sentiment), title=\"Happiest Tweets\", limit=10, headers=headers)\n",
    "\n",
    "display_table(sort_dict(tweet_sentiment, reverse=False), title=\"Unhappiest Tweets\", limit=10, headers=headers)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
