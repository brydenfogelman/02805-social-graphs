{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "\n",
    "Below is some basic initilization, which involves connecting to our remote MongoDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Custom helper file\n",
    "import elections_helper as helper\n",
    "from elections_helper import display_table\n",
    "\n",
    "import numpy as np\n",
    "import re, string, operator, pickle, nltk, pprint, math\n",
    "\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from __future__ import division\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globally Defined Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Twitter users really like these words but we just don't get it.\n",
    "other_stopwords = ['like','get']\n",
    "\n",
    "# retrieve common stop words form nltk\n",
    "stopwords = set(nltk.corpus.stopwords.words('english') + other_stopwords)\n",
    "\n",
    "# Retrieved dicationary containing word mapped with its happiness index\n",
    "words_happiness = pickle.load(open('./data_files/sentiment.pickle','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = helper.setup_mongo_client(properties_file='./properties/db.properties')\n",
    "\n",
    "tweet_collection, _ = helper.get_collections(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "TODO move these into election helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_dict(d, reverse=True):\n",
    "    return sorted(d.items(), key=operator.itemgetter(1), reverse=reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dicts(*dict_args):\n",
    "    '''\n",
    "    Given any number of dicts, shallow copy and merge into a new dict,\n",
    "    precedence goes to key value pairs in latter dicts.\n",
    "    '''\n",
    "    result = {}\n",
    "    for dictionary in dict_args:\n",
    "        result.update(dictionary)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding our Data\n",
    "\n",
    "Initially, let's play around with our data a little bit. We're going to practice making simple queries and displaying some of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet #1: RT @Ethan_Booker: TRUMP: look at this Hillary mask. it's hideous! just like her sou--\n",
      "AIDE: *whispers in ear*\n",
      "TRUMP: this mask is beautiful…\n",
      "Tweet #2: TRUMP: look at this Hillary mask. it's hideous! just like her sou--\n",
      "AIDE: *whispers in ear*\n",
      "TRUMP: this mask is bea… https://t.co/IpquTCKM9F\n",
      "Tweet #3: RT @FoxNews: #DonaldTrump's daughter-in-law, Lara Trump, blasted #HillaryClinton in a new interview. https://t.co/apANzmnyQU #Election2016…\n",
      "Tweet #4: #DonaldTrump's daughter-in-law, Lara Trump, blasted #HillaryClinton in a new interview. https://t.co/apANzmnyQU… https://t.co/eMohElI9aR\n",
      "Tweet #5: WikiLeaks: DNC And CNN Colluded On Questions For Trump, Cruz https://t.co/VDETfDgyLi\n"
     ]
    }
   ],
   "source": [
    "# Query the database and save result to list\n",
    "tweets = [tweet for tweet in tweet_collection.find({}, projection={'text': True}).limit(5)]\n",
    "\n",
    "num = 0\n",
    "for tweet in tweets:\n",
    "    num += 1\n",
    "    print \"Tweet #%s: %s\" % (num, tweet['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a few things we noticed about the data and it's summarized in the list below:\n",
    "* Excess text for a retweeted message. ex. RT @MassDeception1:\n",
    "* Links in tweets\n",
    "* Mentions in tweets\n",
    "* Useless puncuation\n",
    "* Lots of information saved in hashtags!\n",
    "\n",
    "In order to perform any analysis on our data we beed to tokenize it first. Tokenizing is basically converting a string into a list of words. When tokenizing the tweets we can create regular expressions to help handle the potenial issues listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a global variable called all_hashtags, we can \n",
    "all_hashtags = []\n",
    "\n",
    "def tweet_tokenizer(text, extra_stopwords=set([])):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # matches mentions, ampersands and twitter links\n",
    "    remove_pattern = r\"(?:@[\\w]*|https://t.co/[\\w]*|&[\\w]*)\" \n",
    "   \n",
    "    # matches text found after hashtag\n",
    "    hashtag_pattern = r\"#([^\\s]*)\"\n",
    "    \n",
    "    # pattern to remove puncuation except punctuation x\n",
    "    punc_pattern = lambda x:  r\"[^\\w\\d\" + x + \"\\s]+\"\n",
    "    \n",
    "    # match words from hashtag\n",
    "    hashtag_word_pattern = r'([A-Z][^A-Z]*|[a-z][a-z]*)'\n",
    "    \n",
    "    # create a list of all hashtags (not including hash symbol)\n",
    "    hashtags = re.findall(hashtag_pattern, text)\n",
    "    \n",
    "    # add hashtags to global set\n",
    "    all_hashtags.extend(hashtags)\n",
    "    \n",
    "    # remove punctuation from hashtags in case it exists\n",
    "    hashtags = [re.sub(punc_pattern(''),' ', s) for s in hashtags]\n",
    "    \n",
    "    # split hashtag into words\n",
    "    hashtag_tokens = re.findall(hashtag_word_pattern, ' '.join(hashtags))\n",
    "\n",
    "    # remove mentions and links from tweet\n",
    "    text = re.sub(remove_pattern,'', text)\n",
    "    \n",
    "    # replaces punctuation with a space, removes hashtags\n",
    "    text = re.sub(punc_pattern(\"'#\"),' ', text)\n",
    "    \n",
    "    # create a combined string of tweet text and words from hashtag\n",
    "    text = text + ' '.join(hashtag_tokens)\n",
    "    \n",
    "    # split text at whitespace\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # try stemming\n",
    "    # stemmer = SnowballStemmer(\"english\")\n",
    "    # stemmer.stem(t.lower())\n",
    "    \n",
    "    # remove if not in the alphabet and not in stopwords, set to lowercase\n",
    "    return [t.lower() for t in tokens \\\n",
    "                if t.lower() not in stopwords.union(extra_stopwords) and t.isalpha() and len(t) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now testing the tweet_tokenizer function . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet #1: [u'trump', u'look', u'hillary', u'mask', u'hideous', u'sou', u'aide', u'whispers', u'ear', u'trump', u'mask', u'bea']\n",
      "Tweet #2: [u'daughter', u'law', u'lara', u'trump', u'blasted', u'new', u'interview', u'donald', u'trump', u'hillary', u'clinton']\n",
      "Tweet #3: [u'wikileaks', u'dnc', u'cnn', u'colluded', u'questions', u'trump', u'cruz']\n",
      "Tweet #4: [u'hillary', u'clinton', u'person', u'believed', u'never', u'hillary', u'wikileaks', u'johnpodesta', u'podesta', u'emails']\n",
      "Tweet #5: [u'bloomberg', u'poll', u'obama', u'would', u'annihilate', u'trump', u'romney', u'would', u'clobber', u'hillary', u'clinton']\n"
     ]
    }
   ],
   "source": [
    "tweets = [tweet for tweet in tweet_collection.find({'retweeted': False}, projection={'text': True}).limit(5)]\n",
    "\n",
    "num = 0\n",
    "for tweet in tweets:\n",
    "    num += 1\n",
    "    text = tweet['text']\n",
    "    tokens = tweet_tokenizer(text)  \n",
    "    print \"Tweet #%s: %s\" % (num, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Frequency Distribution and TF-IDF\n",
    "\n",
    "For TF-IDF, we will combine all the tweets from the same location and same query since the document size for each individual tweet is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_generator(query, get_string=None):\n",
    "    \n",
    "    # if query is a list we need to aggregate\n",
    "    if type(query) is list:\n",
    "        cur = tweet_collection.aggregate(query)\n",
    "    else:\n",
    "        cur = tweet_collection.find(query)\n",
    "\n",
    "    for document in cur:\n",
    "        _id = document['_id']\n",
    "        if get_string:\n",
    "            result = get_string(document)\n",
    "            if result:\n",
    "                yield (result, _id)\n",
    "        # otherwise yield document\n",
    "        else:\n",
    "            yield document    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bag_words(query, get_string=None, extra_stopwords=set([])):\n",
    "\n",
    "    bag_words = {}\n",
    "    words = []\n",
    "    count = 0\n",
    "    \n",
    "    for text, _id in tweet_generator(query, get_string):\n",
    "        \n",
    "        tokens = tweet_tokenizer(text, extra_stopwords)\n",
    "        \n",
    "        bag_words[_id] = tokens\n",
    "        \n",
    "        words.extend(tokens)\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    print \"Total number of items retrieved is %s\" % count\n",
    "\n",
    "    return bag_words, set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_term_frequency(bag_words):\n",
    "    \n",
    "    tf = {}\n",
    "    for _, tokens in bag_words.iteritems():\n",
    "        for word in tokens:\n",
    "            # if the key does not exist, set to 1 \n",
    "            if tf.get(word,None) is None:\n",
    "                tf[word] = 1\n",
    "            # add one if the key exists\n",
    "            else:\n",
    "                tf[word] += 1\n",
    "        \n",
    "    return tf, sort_dict(tf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tfidf(bag_words, tf, normalize=True, min_frequency=1):\n",
    "    \n",
    "    idf = {}\n",
    "    tfidf = {}\n",
    "    \n",
    "    for _id, tokens in bag_words.iteritems():\n",
    "        \n",
    "        tfidf[_id] = {}\n",
    "        \n",
    "        for word in set(tokens):\n",
    "            \n",
    "            tfidf[_id][word] = None\n",
    "            \n",
    "            # if the key does not exist, set to 1 \n",
    "            if idf.get(word,0) == 0:\n",
    "                idf[word] = 1\n",
    "            # add one if the key exists\n",
    "            else:\n",
    "                idf[word] += 1\n",
    "                \n",
    "    N = len(bag_words)\n",
    "    print \"Size of corpus (number of documents) %s\" % N\n",
    "    \n",
    "    for _id, inner in tfidf.iteritems():\n",
    "        for word in inner.keys():\n",
    "            frequency = tf[word]\n",
    "            if frequency <= min_frequency:\n",
    "                continue\n",
    "            if normalize:\n",
    "                frequency = (1 + math.log(frequency,10))\n",
    "\n",
    "            tfidf[_id][word] = math.log(1 + N / idf[word],10) * frequency\n",
    "    \n",
    "    return tfidf, sort_dict(merge_dicts(*[v for k,v in tfidf.iteritems()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Tweets\n",
    "\n",
    "First, all the tweets will be used to calculate the frequency distribution and TF-IDF (not including retweets). \n",
    "\n",
    "1. Calculating the frequency distribution.\n",
    "2. Calculating the TF-DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of items retrieved is 71370\n",
      "Total number of unique words is 34078\n",
      "\n",
      "Most frequent words: \n",
      "[(u'trump', 46373), (u'election', 14873), (u'day', 9544), (u'president', 8609), (u'clinton', 7593), (u'donald', 7477), (u'hillary', 5694), (u'america', 5619), (u'night', 4975), (u'vote', 4925)]\n"
     ]
    }
   ],
   "source": [
    "query = {'retweeted': False}\n",
    "get_string = lambda x: x['text']\n",
    "# Find a bag of words representation\n",
    "bag_words, unique_words = get_bag_words(query, get_string)\n",
    "\n",
    "# Find the frequency distribution\n",
    "tf, tf_sorted = get_term_frequency(bag_words)\n",
    "print \"Total number of unique words is %s\\n\" % len(unique_words)\n",
    "\n",
    "print \"Most frequent words: \"\n",
    "print tf_sorted[:10]\n",
    "# display_table(tf_sorted, title=\"Most frequent words from Tweets\", headers=['Words','Frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the frequent words contain the words we used in our queries when getting Twitter data. Let's take a look at what are the most popular words when we ignore these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_stopwords = [u'financial', u'mexico', u'narrates', u'new', u'final', u'market', u'trump ', \\\n",
    "                   u'riot', u'cabinet', u'hillary', u'president', u'america', u'day', u'thoughts', \\\n",
    "                   u'stock', u'clinton ', u'positions', u'weed', u'planet', u'electionnight', u'clinton', \\\n",
    "                   u'obama', u'elections2016 ', u'elect', u'trump', u'canadian', u'donald', u'election', \\\n",
    "                   u'earth', u'still','night']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of items retrieved is 71370\n",
      "Most frequent words: \n",
      "[(u'vote', 4925), (u'people', 3456), (u'win', 2581), (u'voted', 2540), (u'world', 2092)]\n"
     ]
    }
   ],
   "source": [
    "# Find a bag of words representation\n",
    "bag_words, unique_words = get_bag_words(query, get_string, query_stopwords)\n",
    "\n",
    "# Find the frequency distribution\n",
    "_, tf_sorted = get_term_frequency(bag_words)\n",
    "print \"Most frequent words: \"\n",
    "print tf_sorted[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will calculate the TF-IDF in three ways:\n",
    "\n",
    "1. Smooth IDF and TF \n",
    "> ( 1 + log(size/occurences) ) * tf\n",
    "2. Smooth IDF and normalized TF\n",
    "> ( 1 + log(size/occurences) ) * (1 + log(tf))\n",
    "3. Smooth IDF and normalized TF with a minimum TF of 100\n",
    "> ( 1 + log(size/occurences) ) * (1 + log(tf)) if tf >= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth IDF and TF\n",
      "Size of corpus (number of documents) 71370\n",
      "[(u'vote', 6104.075305499893), (u'people', 4685.963054884916), (u'win', 3806.7569528162117), (u'voted', 3797.525176872505), (u'world', 3260.189736881924), (u'wins', 3232.526477994719), (u'one', 3014.9385702047366), (u'white', 2838.4426995117924), (u'going', 2821.594692643993), (u'would', 2811.2446362677156)]\n",
      "\n",
      "Smooth IDF and normalized TF\n",
      "Size of corpus (number of documents) 71370\n",
      "[(u'dubai', 9.298709173761077), (u'blah', 9.090160063745648), (u'counting', 9.020739498696368), (u'bye', 8.949604717595758), (u'gang', 8.881102413833117), (u'shirt', 8.869836422815819), (u'biden', 8.861901883759453), (u'neck', 8.846747184391567), (u'choosing', 8.769699129411945), (u'death', 8.750707003324493)]\n",
      "\n",
      "Minimum word frequency of 100 TF-IDF:\n",
      "Size of corpus (number of documents) 71370\n",
      "[(u'dubai', 9.298709173761077), (u'counting', 9.020739498696368), (u'bye', 8.949604717595758), (u'biden', 8.861901883759453), (u'death', 8.750707003324493), (u'wearing', 8.743742101975677), (u'leaving', 8.677808105508392), (u'chose', 8.67633675346131), (u'podesta', 8.666231121050155), (u'texas', 8.666033983638451)]\n"
     ]
    }
   ],
   "source": [
    "print 'Smooth IDF and TF'\n",
    "tfidf, tfidf_sorted = get_tfidf(bag_words, tf, normalize=False)\n",
    "print tfidf_sorted[:10]\n",
    "\n",
    "print '\\nSmooth IDF and normalized TF'\n",
    "_, tfidf_sorted = get_tfidf(bag_words, tf)\n",
    "print tfidf_sorted[:10]\n",
    "\n",
    "print '\\nMinimum word frequency of 100 TF-IDF:'\n",
    "_, tfidf_sorted = get_tfidf(bag_words, tf, min_frequency=100)\n",
    "print tfidf_sorted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that not normalizing the term frequency will cause a few issues. When not normalized the highest TF-IDF values generally tend to be the most frequent values. By normalizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places\n",
    "\n",
    "Next, tweets that have a location of origin will be used and retweets will not be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of items retrieved is 12\n",
      "Total number of unique words is 25334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = [\n",
    "    {\n",
    "        '$match': {'retweeted': False, 'place': {'$ne': None}}\n",
    "    },\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$place.country\",\n",
    "            'text': {'$push': '$text'}\n",
    "        }\n",
    "    },\n",
    "        {\n",
    "        \"$project\": {\n",
    "            \"location\": \"$_id.location\",\n",
    "            'all_text': '$text'\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "get_string = lambda x: ' '.join(x['all_text']) if len(x['all_text']) > 1000 else None\n",
    "\n",
    "# Find a bag of words representation\n",
    "bag_words, unique_words = get_bag_words(query, get_string, query_stopwords)\n",
    "\n",
    "# Find the frequency distribution\n",
    "tf, tf_sorted = get_term_frequency(bag_words)\n",
    "print \"Total number of unique words is %s\\n\" % len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of corpus (number of documents) 12\n",
      "Canada\n",
      "[(u'dubai', 2.5929178581611345), (u'impacts', 2.198995798670775), (u'vancouver', 1.9958915652655798), (u'cst', 1.9614356285831662), (u'brazil', 1.8677162513475947), (u'toronto', 1.699450340776646), (u'heinous', 1.6507769157817132), (u'sweden', 1.614268076122521), (u'samuel', 1.5763678326709614), (u'popovich', 1.5406137538341171)]\n",
      "\n",
      "United Kingdom\n",
      "[(u'zuma', 2.5960414635294673), (u'choosewisely', 2.080967721759438), (u'brazilian', 1.9958915652655798), (u'syllabus', 1.7697669357399435), (u'swedish', 1.623161630509354), (u'presidente', 1.4564449945154023), (u'imstillwithher', 1.4163263090279765), (u'fights', 1.4095318928498548), (u'australia', 1.3902425212688523), (u'department', 1.3544752401323321)]\n",
      "\n",
      "Australia\n",
      "[(u'auspol', 3.034688994791726), (u'qand', 2.5632170671285213), (u'turnbull', 2.4552648023662274), (u'qanda', 2.180170061161796), (u'poori', 2.021487376982741), (u'abbott', 1.8626974772987794), (u'heinous', 1.6507769157817132), (u'hai', 1.5776861027558453), (u'popovich', 1.5406137538341171), (u'prophet', 1.5406137538341171)]\n",
      "\n",
      "South Africa\n",
      "[(u'sast', 3.2799868986243514), (u'joshua', 2.690143506970278), (u'zuma', 2.5960414635294673), (u'anc', 2.58682074596752), (u'jacob', 2.0115118551903786), (u'rand', 1.9257702018406653), (u'brazil', 1.8677162513475947), (u'hai', 1.5776861027558453), (u'samuel', 1.5763678326709614), (u'prophet', 1.5406137538341171)]\n",
      "\n",
      "United Arab Emirates\n",
      "[(u'dubai', 2.5929178581611345), (u'polifocus', 2.58682074596752), (u'surgical', 2.040889243056806), (u'poori', 2.021487376982741), (u'pak', 1.9059263737608463), (u'syllabus', 1.7697669357399435), (u'narendra', 1.7510246912086418), (u'hai', 1.5776861027558453), (u'modified', 1.4102789789022396), (u'fights', 1.4095318928498548)]\n",
      "\n",
      "Mexico\n",
      "[(u'impacts', 2.198995798670775), (u'cst', 1.9614356285831662), (u'feliz', 1.8626974772987794), (u'elecciones', 1.559976334836564), (u'por', 1.540027708471343), (u'hiring', 1.5241021748413797), (u'presidente', 1.4564449945154023), (u'imstillwithher', 1.4163263090279765), (u'nos', 1.4102789789022396), (u'los', 1.4102789789022396)]\n",
      "\n",
      "India\n",
      "[(u'monetisation', 2.609326173594656), (u'beasts', 2.4552648023662274), (u'qanda', 2.180170061161796), (u'surgical', 2.040889243056806), (u'poori', 2.021487376982741), (u'pak', 1.9059263737608463), (u'blackmoney', 1.8626974772987794), (u'ist', 1.854515962257931), (u'syllabus', 1.7697669357399435), (u'narendra', 1.7510246912086418)]\n",
      "\n",
      "France\n",
      "[(u'cet', 2.131311934527243), (u'blackmoney', 1.8626974772987794), (u'ist', 1.854515962257931), (u'por', 1.540027708471343), (u'presidente', 1.4564449945154023), (u'modified', 1.4102789789022396), (u'nos', 1.4102789789022396), (u'fights', 1.4095318928498548), (u'australia', 1.3902425212688523), (u'cooking', 1.3719463732282229)]\n",
      "\n",
      "United States\n",
      "[(u'choosewisely', 2.080967721759438), (u'jacob', 2.0115118551903786), (u'cst', 1.9614356285831662), (u'rand', 1.9257702018406653), (u'abbott', 1.8626974772987794), (u'feliz', 1.8626974772987794), (u'toronto', 1.699450340776646), (u'swedish', 1.623161630509354), (u'sweden', 1.614268076122521), (u'samuel', 1.5763678326709614)]\n",
      "\n",
      "Sweden\n",
      "[(u'kvalvaka', 2.5632170671285213), (u'macken', 2.5122458147002136), (u'cet', 2.131311934527243), (u'swedish', 1.623161630509354), (u'sweden', 1.614268076122521), (u'sarkar', 1.5927804143387172), (u'hiring', 1.5241021748413797), (u'africans', 1.4638268467646813), (u'los', 1.4102789789022396), (u'australia', 1.3902425212688523)]\n",
      "\n",
      "Germany\n",
      "[(u'cet', 2.131311934527243), (u'vancouver', 1.9958915652655798), (u'ist', 1.854515962257931), (u'syllabus', 1.7697669357399435), (u'narendra', 1.7510246912086418), (u'toronto', 1.699450340776646), (u'sarkar', 1.5927804143387172), (u'hai', 1.5776861027558453), (u'por', 1.540027708471343), (u'disastrous', 1.4437037408260607)]\n",
      "\n",
      "Brazil\n",
      "[(u'ganhou', 2.944656536109503), (u'fora', 2.860831237907386), (u'temer', 2.8199241111441307), (u'paulo', 2.690143506970278), (u'mas', 2.6711693318908147), (u'assisto', 2.58682074596752), (u'odeio', 2.58682074596752), (u'eleicoes', 2.5122458147002136), (u'brazilian', 1.9958915652655798), (u'brazil', 1.8677162513475947)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf, tfidf_sorted = get_tfidf(bag_words, tf, normalize=True, min_frequency=15)\n",
    "\n",
    "for place, values in tfidf.iteritems():\n",
    "    print place\n",
    "    print sort_dict(values)[:10]\n",
    "    print\n",
    "# display_table(tfidf_sorted, title=\"Highest TF-IDF words from Tweets\", limit=15, headers=['Words','TF-IDF'])\n",
    "\n",
    "# tfidf_sorted.reverse()\n",
    "# display_table(tfidf_sorted, title=\"Lowest TF-IDF words from Tweets\", limit=15, headers=['Words','Frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of items retrieved is 13\n",
      "Total number of unique words is 33675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = [{'$match': {'retweeted': False}}, \n",
    "         {\"$group\": { \"_id\": \"$root_query\", 'text': {'$push': '$text'}}},\n",
    "         {\"$project\": {'all_text': '$text'}}]\n",
    "\n",
    "get_string = lambda x: ' '.join(x['all_text'])\n",
    "\n",
    "# Find a bag of words representation\n",
    "bag_words, unique_words = get_bag_words(query, get_string)\n",
    "\n",
    "# Find the frequency distribution\n",
    "tf, tf_sorted = get_term_frequency(bag_words)\n",
    "\n",
    "print \"Total number of unique words is %s\\n\" % len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of corpus (number of documents) 13\n",
      "#NewTrumpCabinetPositions\n",
      "[(u'department', 2.164801273190773), (u'bannon', 2.087645828010348), (u'french', 2.0638396777689993), (u'positions', 2.057262581619161), (u'defence', 2.013542215107921), (u'popovich', 1.9940490050081678), (u'salary', 1.9288821224592476), (u'gregg', 1.928740016689785), (u'article', 1.9185206964602164), (u'blind', 1.9070988971789762)]\n",
      "\n",
      "stock OR market OR financial OR obama OR weed OR canadian OR mexico\n",
      "[(u'concern', 2.127592247229762), (u'final', 2.1209754283788196), (u'thoughts', 2.117053452658006), (u'fourth', 2.0666565439729006), (u'difficult', 2.0666565439729006), (u'pak', 2.0320841499224453), (u'japan', 2.013542215107921), (u'kentucky', 1.9805450917472285), (u'however', 1.9805450917472285), (u'brazil', 1.974530975893134)]\n",
      "\n",
      "#TrumpNarratesPlanetEarth\n",
      "[(u'narrates', 3.052695597386898), (u'trumpnarratesplanetearth', 2.5563800409399255), (u'hotels', 2.1921597050143062), (u'bannon', 2.087645828010348), (u'sue', 1.9735016076082625), (u'planet', 1.83826081920269), (u'transition', 1.7928052321633274), (u'nafta', 1.7901602121453402), (u'ivanka', 1.7170059912113027), (u'elephant', 1.7169731325164517)]\n",
      "\n",
      "#TrumpRiot\n",
      "[(u'cohen', 2.140635157083539), (u'leonard', 2.1025490972273086), (u'resist', 2.0828306563112844), (u'portland', 2.0449844320742776), (u'flash', 1.928740016689785), (u'blm', 1.928740016689785), (u'comedy', 1.8916943037092704), (u'protesters', 1.8493066431389238), (u'rioters', 1.8212408475281265), (u'coz', 1.8008640011250727)]\n",
      "\n",
      "#Elections2016 OR #ElectionDay\n",
      "[(u'zuma', 2.700753932711638), (u'dubai', 2.6848505965001475), (u'cet', 2.671630679988566), (u'channels', 2.5851431826754414), (u'syllabus', 2.580896945449551), (u'sast', 2.576602727126695), (u'por', 2.549765630955131), (u'poori', 2.545102575175783), (u'paris', 2.4555793933388275), (u'eric', 2.39798523722031)]\n",
      "\n",
      "#NotMyPresident\n",
      "[(u'thoughts', 2.117053452658006), (u'stump', 2.0320841499224453), (u'claimed', 2.0320841499224453), (u'crack', 1.9735016076082625), (u'remarkable', 1.928740016689785), (u'imstillwithher', 1.908685187147373), (u'xenophobic', 1.811216782246648), (u'france', 1.8102713709277207), (u'pres', 1.7570049526821885), (u'trump', 1.7104825170104658)]\n",
      "\n",
      "ElectionNight OR America\n",
      "[(u'farage', 2.2411810427205783), (u'troll', 2.2369212959327838), (u'final', 2.1209754283788196), (u'erdogan', 2.0828306563112844), (u'positions', 2.057262581619161), (u'nigel', 1.9426117436404777), (u'jews', 1.9358215564348242), (u'echo', 1.928740016689785), (u'voter', 1.9185239501751434), (u'obvious', 1.8994905402076492)]\n",
      "\n",
      "stock market OR financial OR obama\n",
      "[(u'attack', 1.908685187147373), (u'blockchain', 1.8403819524871865), (u'dropping', 1.8212408475281265), (u'effect', 1.8173602329889513), (u'electionday', 1.8062030586117366), (u'uncertainty', 1.779080775844801), (u'chief', 1.765251456518164), (u'early', 1.7392008281822666), (u'virginia', 1.7304288347004635), (u'asia', 1.7169731325164517)]\n",
      "\n",
      "ElectionNight\n",
      "[(u'remarks', 2.167632854729421), (u'final', 2.1209754283788196), (u'thoughts', 2.117053452658006), (u'hampshire', 2.040089271446499), (u'loses', 1.9994352878160595), (u'conversion', 1.9940490050081678), (u'demographic', 1.9940490050081678), (u'carolina', 1.9827782365318596), (u'steve', 1.9394343517479653), (u'jews', 1.9358215564348242)]\n",
      "\n",
      "#ImStillWithHer\n",
      "[(u'imstillwithher', 1.908685187147373), (u'speaker', 1.8916943037092704), (u'kid', 1.799280388459691), (u'headed', 1.7675983636939059), (u'blocked', 1.7556825519747614), (u'threats', 1.7119047988603218), (u'trump', 1.7104825170104658), (u'cast', 1.702673862681749), (u'believing', 1.7014071259943155), (u'hotel', 1.7014071259943155)]\n",
      "\n",
      "#PresidentElectTrump\n",
      "[(u'pepsi', 2.180094111372304), (u'clearance', 2.167632854729421), (u'adviser', 2.154749103014653), (u'unfollow', 2.140635157083539), (u'cohen', 2.140635157083539), (u'final', 2.1209754283788196), (u'thoughts', 2.117053452658006), (u'liars', 2.1132496289326275), (u'thiel', 2.1132496289326275), (u'leonard', 2.1025490972273086)]\n",
      "\n",
      "#DonaldTrump OR #HillaryClinton OR Trump OR Clinton OR #Trump OR #Clinton\n",
      "[(u'narrates', 3.052695597386898), (u'sweeping', 2.9567623060653636), (u'qanda', 2.9567623060653636), (u'temer', 2.9013989585505127), (u'agreement', 2.855419484318282), (u'buhari', 2.8390981178086583), (u'corbyn', 2.8390981178086583), (u'forbes', 2.8047564274564807), (u'kennedy', 2.8047564274564807), (u'fewer', 2.767868659525091)]\n",
      "\n",
      "#ElectionFinalThoughts\n",
      "[(u'instantwingame', 2.684716305898907), (u'strategist', 2.5208883816572105), (u'kremlin', 2.226215397043603), (u'australia', 2.2052952115306517), (u'warns', 2.1921597050143062), (u'choosewisely', 2.154749103014653), (u'rant', 2.154749103014653), (u'former', 2.1512193281281813), (u'cases', 2.141413198625467), (u'debate', 2.1259473210973487)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf, tfidf_sorted = get_tfidf(bag_words, tf, normalize=True, min_frequency=15)\n",
    "\n",
    "for query, values in tfidf.iteritems():\n",
    "    print query\n",
    "    print sort_dict(values)[:10]\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to make sure this is handled properly\n",
    "tf['imstillwithher']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sentiment(tokens, min_sentiment_tokens=3):\n",
    "    \n",
    "    combined_happiness = 0\n",
    "    words_with_no_sentiment = set()\n",
    "    words_sentiment_count = 0\n",
    "    \n",
    "    # Go through each token and if we have a sentiment for it, add it to the combined happiness score\n",
    "    for token in tokens:\n",
    "        # get the happiness value, otherwise return zero\n",
    "        happiest_value = words_happiness.get(token,0)\n",
    "        \n",
    "        # if a happiness value exists keep track \n",
    "        if happiest_value != 0:\n",
    "            combined_happiness += happiest_value\n",
    "            words_sentiment_count += 1\n",
    "        # save the words that have no happiness index\n",
    "        else:\n",
    "            words_with_no_sentiment.add(token)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Safe check to avoid division by 0\n",
    "    if combined_happiness == 0: \n",
    "        avg_sentiment_score = 0 \n",
    "    else: \n",
    "        avg_sentiment_score = combined_happiness / words_sentiment_count\n",
    "    \n",
    "    # if the tweet does not have a minimum of 4 words that have sentiment, return a score of zero\n",
    "    if words_sentiment_count <= min_sentiment_tokens:\n",
    "        return 0, words_with_no_sentiment\n",
    "    \n",
    "    return avg_sentiment_score, words_with_no_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_sentiment = {}\n",
    "tweets_with_no_sentiment = []\n",
    "all_words_with_no_sentiment = set()\n",
    "\n",
    "for tweet in tweet_generator({'retweeted': False}):\n",
    "    text = tweet['text']\n",
    "    _id = tweet['_id']\n",
    "    \n",
    "    tokens = tweet_tokenizer(text)\n",
    "    \n",
    "    avg_sentiment_score, words_with_no_sentiment = get_sentiment(tokens)\n",
    "    \n",
    "    # update set of words with no sentiment\n",
    "    if len(words_with_no_sentiment) > 0:\n",
    "        all_words_with_no_sentiment = all_words_with_no_sentiment.union(words_with_no_sentiment)\n",
    "    \n",
    "    # If we didn't find any sentiment for the tweet, save the tweet id\n",
    "    if avg_sentiment_score == 0:\n",
    "        tweets_with_no_sentiment.append(_id)\n",
    "    # otherwise save the sentiment to the dictionary\n",
    "    else:\n",
    "        tweet_sentiment[_id] = avg_sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average sentiment of tweet sentiment is   5.57\n",
      "The stanard deviation of tweet sentiment is   0.45\n",
      "\n",
      "Percentage of tweets that have a sentiment    86.68%\n",
      "Percentage of words that have a sentiment     23.01%\n",
      "\n",
      "The happiest tweet is                         7.53\n",
      "Tweet text: @realDonaldTrump Congratulations For Great victory All Indian with you #electionday\n",
      "\n",
      "The saddest tweet is                          2.62\n",
      "Tweet text: Is this the End of Terrorism ( #Elections2016 ) and Corruption? ( #ModiFightsCorruption )\n"
     ]
    }
   ],
   "source": [
    "values = tweet_sentiment.values()\n",
    "\n",
    "average_sentiment = np.average(values)\n",
    "std_sentiment = np.std(values)\n",
    "happiest_value = max(values)\n",
    "saddest_value = min(values)\n",
    "\n",
    "tweet_percent = (len(tweet_sentiment) / (len(tweets_with_no_sentiment) + len(tweet_sentiment))) * 100\n",
    "word_percent = (1 - len(all_words_with_no_sentiment) / len(unique_words)) * 100\n",
    "\n",
    "happiest_tweet_id = tweet_sentiment.keys()[values.index(happiest_value)]\n",
    "saddest_tweet_id = tweet_sentiment.keys()[values.index(saddest_value)]\n",
    "\n",
    "happy_tweet = tweet_collection.find_one({'_id': happiest_tweet_id})['text']\n",
    "sad_tweet = tweet_collection.find_one({'_id': saddest_tweet_id})['text']\n",
    "\n",
    "print \"The average sentiment of tweet sentiment is   %2.2f\" % average_sentiment\n",
    "print \"The stanard deviation of tweet sentiment is   %2.2f\" % std_sentiment\n",
    "print ''\n",
    "print \"Percentage of tweets that have a sentiment    %2.2f%%\" % tweet_percent\n",
    "print \"Percentage of words that have a sentiment     %2.2f%%\" % word_percent\n",
    "print ''\n",
    "print \"The happiest tweet is                         %2.2f\" % happiest_value\n",
    "print \"Tweet text: %s\" % happy_tweet\n",
    "print ''\n",
    "print \"The saddest tweet is                          %2.2f\" % saddest_value\n",
    "print \"Tweet text: %s\" % sad_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happiest Tweets (limited to 10 results)\n",
      "╒════════════════════╤═══════════════════╕\n",
      "│           Tweet ID │   Happiness Index │\n",
      "╞════════════════════╪═══════════════════╡\n",
      "│ 796291278740697088 │           7.53    │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 796291597507796992 │           7.53    │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 795703881317552128 │           7.29    │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 796809782812442624 │           7.26    │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 797587597136633856 │           7.23333 │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 796302267833991168 │           7.21    │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 796329231420706816 │           7.19    │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 796380493684084736 │           7.185   │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 796299322971168768 │           7.17556 │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 796268118528114690 │           7.17    │\n",
      "╘════════════════════╧═══════════════════╛\n",
      "\n",
      "\n",
      "Unhappiest Tweets (limited to 10 results)\n",
      "╒════════════════════╤═══════════════════╕\n",
      "│           Tweet ID │   Happiness Index │\n",
      "╞════════════════════╪═══════════════════╡\n",
      "│ 796272419014967297 │            2.624  │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 795053056991854592 │            2.995  │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 796279651308077056 │            3.1025 │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 796330966805454848 │            3.11   │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 796410081021673472 │            3.12   │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 796304196999979008 │            3.19   │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 796116612776624130 │            3.21   │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 796327473206075396 │            3.23   │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 796324439986237440 │            3.235  │\n",
      "├────────────────────┼───────────────────┤\n",
      "│ 796292966679805952 │            3.235  │\n",
      "╘════════════════════╧═══════════════════╛\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "headers = ['Tweet ID','Happiness Index']\n",
    "\n",
    "display_table(sort_dict(tweet_sentiment), title=\"Happiest Tweets\", limit=10, headers=headers)\n",
    "\n",
    "display_table(sort_dict(tweet_sentiment, reverse=False), title=\"Unhappiest Tweets\", limit=10, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61863\n",
      "71370\n",
      "9507\n",
      "25926\n"
     ]
    }
   ],
   "source": [
    "# for debugging\n",
    "print len(tweet_sentiment)\n",
    "cur = tweet_collection.find({'retweeted': False})\n",
    "print cur.count()\n",
    "print len(tweets_with_no_sentiment)\n",
    "print len(all_words_with_no_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment of Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named __builtin__\r",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e2009de9b453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcommunities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data_files/community.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGLOBAL\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;31m# Subclasses may override this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named __builtin__\r"
     ]
    }
   ],
   "source": [
    "communities = pickle.load(open('./data_files/community.txt','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment of Notable Twitter Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
