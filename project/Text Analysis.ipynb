{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "\n",
    "Below is some basic initilization, which involves connecting to our remote MongoDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Custome helper file\n",
    "import elections_helper as helper\n",
    "from elections_helper import display_table\n",
    "\n",
    "import numpy as np\n",
    "import re, string, operator, pickle, nltk, pprint\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from __future__ import division\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globally Defined Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# retrieve common stop words form nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Retrieved dicationary containing word mapped with its happiness index\n",
    "words_happiness = pickle.load(open('./data_files/sentiment.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = helper.setup_mongo_client(properties_file='./properties/db.properties')\n",
    "\n",
    "tweet_collection, _ = helper.get_collections(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding our Data\n",
    "\n",
    "Initially, let's display some of the content in the tweets, practice making queries and looking and commonly used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet #1: RT @Ethan_Booker: TRUMP: look at this Hillary mask. it's hideous! just like her sou--\n",
      "AIDE: *whispers in ear*\n",
      "TRUMP: this mask is beautifulâ€¦\n",
      "Tweet #2: TRUMP: look at this Hillary mask. it's hideous! just like her sou--\n",
      "AIDE: *whispers in ear*\n",
      "TRUMP: this mask is beaâ€¦ https://t.co/IpquTCKM9F\n",
      "Tweet #3: RT @FoxNews: #DonaldTrump's daughter-in-law, Lara Trump, blasted #HillaryClinton in a new interview. https://t.co/apANzmnyQU #Election2016â€¦\n",
      "Tweet #4: #DonaldTrump's daughter-in-law, Lara Trump, blasted #HillaryClinton in a new interview. https://t.co/apANzmnyQUâ€¦ https://t.co/eMohElI9aR\n",
      "Tweet #5: WikiLeaks: DNC And CNN Colluded On Questions For Trump, Cruz https://t.co/VDETfDgyLi\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "cur = tweet_collection.find({}, projection={'text': True}, limit = N)\n",
    "\n",
    "for i in xrange(N):\n",
    "    tweet_text = cur.next()\n",
    "    print \"Tweet #%s: %s\" % (i+1, tweet_text['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note:\n",
    "* Excess text for a retweeted message. ex. RT @MassDeception1:\n",
    "* Links in tweets\n",
    "* Mentions in tweets\n",
    "\n",
    "First off, we don't need to include retweeted message since they will have the same sentiment as the original. We can use regular expressions to filter out links in the format https://t.co/... and mentions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_hashtags = set([])\n",
    "\n",
    "def tweet_tokenizer(text):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # matches mentions and twitter links\n",
    "    remove_pattern = r\"(?:@[\\w]*|https://t.co/[\\w]*)\" \n",
    "   \n",
    "    # matches text found after hashtag\n",
    "    hashtag_pattern = r\"#([^\\s]*)\"\n",
    "    \n",
    "    # pattern to remove puncuation except punctuation x\n",
    "    punc_pattern = lambda x:  r\"[^\\w\\d\" + x + \"\\s]+\"\n",
    "    \n",
    "    # match words from hashtag\n",
    "    hashtag_word_pattern = r'([A-Z][^A-Z]*|[a-z][a-z]*)'\n",
    "    \n",
    "    # create a list of all hashtags (not including hash symbol)\n",
    "    hashtags = re.findall(hashtag_pattern, text)\n",
    "    \n",
    "    # add hashtags to global set\n",
    "    all_hashtags.update(hashtags)\n",
    "    \n",
    "    # remove punctuation from hashtags in case it exists\n",
    "    hashtags = [re.sub(punc_pattern(''),' ', s) for s in hashtags]\n",
    "    \n",
    "    # split hashtag into words\n",
    "    hashtag_tokens = re.findall(hashtag_word_pattern, ' '.join(hashtags))\n",
    "\n",
    "    # remove mentions and links from tweet\n",
    "    text = re.sub(remove_pattern,'', text)\n",
    "    \n",
    "    # replaces punctuation with a space, removes hashtags\n",
    "    text = re.sub(punc_pattern(\"'#\"),' ', text)\n",
    "    \n",
    "    # create a combined string of tweet text and words from hashtag\n",
    "    text = text + ' '.join(hashtag_tokens)\n",
    "    \n",
    "    # split text at whitespace\n",
    "    tokens = set(text.split())\n",
    "    \n",
    "    # remove if not in the alphabet and not in stopwords, set to lowercase\n",
    "    return [t.lower() for t in tokens if t.lower() not in stopwords and t.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet #1: [u'ear', u'trump', u'bea', u'mask', u'like', u'whispers', u'hillary', u'sou', u'aide', u'hideous', u'look']\n",
      "TRUMP: look at this Hillary mask. it's hideous! just like her sou--\n",
      "AIDE: *whispers in ear*\n",
      "TRUMP: this mask is beaâ€¦ https://t.co/IpquTCKM9F\n",
      "Tweet #2: [u'daughter', u'trump', u'hillary', u'donald', u'lara', u'clinton', u'interview', u'new', u'law', u'blasted']\n",
      "#DonaldTrump's daughter-in-law, Lara Trump, blasted #HillaryClinton in a new interview. https://t.co/apANzmnyQUâ€¦ https://t.co/eMohElI9aR\n",
      "Tweet #3: [u'dnc', u'cruz', u'questions', u'cnn', u'wikileaks', u'colluded', u'trump']\n",
      "WikiLeaks: DNC And CNN Colluded On Questions For Trump, Cruz https://t.co/VDETfDgyLi\n",
      "Tweet #4: [u'never', u'clinton', u'believed', u'emails', u'person', u'hillary', u'johnpodesta', u'wikileaks', u'podesta']\n",
      "Hillary Clinton is not a person who can be believed #NeverHillary #wikileaks #johnpodesta #PodestaEmailsâ€¦ https://t.co/ity0m0nJLL\n",
      "Tweet #5: [u'obama', u'clinton', u'annihilate', u'would', u'romney', u'hillary', u'clobber', u'bloomberg', u'poll', u'trump']\n",
      "Bloomberg Poll:\n",
      "Obama would annihilate Trump 53-41. \n",
      "Romney would clobber Hillary Clinton 50-40.\n",
      "https://t.co/7TD7WIO2vR\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "cur = tweet_collection.find({'retweeted': False}, projection={'text': True}, limit = N)\n",
    "\n",
    "for i in xrange(N):\n",
    "    tweet = cur.next()\n",
    "    text = tweet['text']\n",
    "    tokens = tweet_tokenizer(text)\n",
    "    \n",
    "    print \"Tweet #%s: %s\" % (i+1, tokens)\n",
    "    print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_iterable(text_only=True):\n",
    "    cur = tweet_collection.find({'retweeted': False})\n",
    "    for tweet in cur:\n",
    "        if text_only:\n",
    "            yield tweet['text']\n",
    "        else:\n",
    "            yield tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dicts(*dict_args):\n",
    "    '''\n",
    "    Given any number of dicts, shallow copy and merge into a new dict,\n",
    "    precedence goes to key value pairs in latter dicts.\n",
    "    '''\n",
    "    result = {}\n",
    "    for dictionary in dict_args:\n",
    "        result.update(dictionary)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dict_representation(X, feature_names, merge=False):\n",
    "    '''\n",
    "    Convert sparse matrix representation to dictionary.\n",
    "    '''\n",
    "    dict_vectorizer = DictVectorizer()\n",
    "\n",
    "    # set feature names so dictionaries can be unpacked\n",
    "    dict_vectorizer.feature_names_ = feature_names\n",
    "\n",
    "    # merge dictionaries\n",
    "    if merge:\n",
    "        return merge_dicts(*dict_vectorizer.inverse_transform(X))\n",
    "    # keep seperate\n",
    "    else:\n",
    "        return dict_vectorizer.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def display_table(data, title=None, limit=20, **kwargs):\n",
    "#     if title:            \n",
    "#         print title + ' (limited to %s results)' % limit\n",
    "#     if type(data[0]) == tuple:\n",
    "#         data = [[str(tup[0]), str(tup[1])] for tup in data[:limit]]\n",
    "#     print tabulate(data[:limit], tablefmt=\"fancy_grid\", **kwargs)\n",
    "#     print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_dict(d, reverse=True):\n",
    "    return sorted(d.items(), key=operator.itemgetter(1), reverse=reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a count vectorizer to convert tweets to bag of word representation\n",
    "count_vectorizer = CountVectorizer(tokenizer=tweet_tokenizer)\n",
    "\n",
    "# Create a matrix passing in a function that iterates over each tweet\n",
    "count = count_vectorizer.fit_transform(tweet_iterable())\n",
    "\n",
    "# Save the indices of\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "\n",
    "# Get a dictionary representation of the bag of words matrix\n",
    "# This can be used to create a tweet id to token dictionary\n",
    "count_dict = get_dict_representation(count, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words is 36971\n",
      "\n",
      "Most frequent words from Tweets (limited to 20 results)\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ Words                 â”‚   Frequency â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ trump                 â”‚       39772 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ clinton               â”‚        6660 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ electionday           â”‚        6602 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ donald                â”‚        6514 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ president             â”‚        5959 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ america               â”‚        4803 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ hillary               â”‚        4767 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ election              â”‚        4617 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ vote                  â”‚        4176 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ people                â”‚        3268 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ amp                   â”‚        3212 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ us                    â”‚        3187 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ like                  â”‚        3128 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ elections             â”‚        3076 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ electionnight         â”‚        2927 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ electionfinalthoughts â”‚        2777 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ obama                 â”‚        2658 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ win                   â”‚        2462 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ voted                 â”‚        2267 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ get                   â”‚        2002 â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_words = merge_dicts(*count_dict).keys()\n",
    "\n",
    "freq_dist = {}\n",
    "for tweet_tokens in count_dict:\n",
    "    for k in tweet_tokens.keys():\n",
    "        if freq_dist.get(k,0) == 0:\n",
    "            freq_dist[k] = 1\n",
    "        else:\n",
    "            freq_dist[k] += 1\n",
    "            \n",
    "print \"Total number of unique words is %s\\n\" % len(unique_words)\n",
    "sorted_freq_dist = sort_dict(freq_dist)\n",
    "display_table(sorted_freq_dist, title=\"Most frequent words from Tweets\", headers=['Words','Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a transformer to convert to a tf-idf representation\n",
    "tfidf_transformer = TfidfTransformer(sublinear_tf=True)\n",
    "\n",
    "# Calculate tf-idf using bag of words matrix\n",
    "tfidf = tfidf_transformer.fit_transform(count)\n",
    "\n",
    "# Create a dictionary representation of the sparse matrix\n",
    "tfidf_dict = get_dict_representation(tfidf, feature_names, merge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest TF-IDF words from Tweets (limited to 20 results)\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ Words                       â”‚   TF-IDF â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ oceans                      â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ trumpette                   â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ tempting                    â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ safe                        â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ happenelectionfinalthoughts â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ kitchen                     â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ nervio                      â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ clintonclintonvstrump       â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ bawbag                      â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ triumphedtrump              â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ ughh                        â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ bbz                         â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ trumpvote                   â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ missionary                  â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ whammy                      â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ kip                         â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ trumpdopa                   â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ brainer                     â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ sooo                        â”‚        1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ hairforceone                â”‚        1 â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•›\n",
      "\n",
      "\n",
      "Highest TF-IDF words from Tweets (limited to 20 results)\n",
      "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚ Words                       â”‚   Frequency â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ oceans                      â”‚           6 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ trumpette                   â”‚           1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ tempting                    â”‚           2 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ safe                        â”‚         165 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ happenelectionfinalthoughts â”‚           1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ kitchen                     â”‚           4 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ nervio                      â”‚           1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ clintonclintonvstrump       â”‚           1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ bawbag                      â”‚           1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ triumphedtrump              â”‚           1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ ughh                        â”‚           1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ bbz                         â”‚           1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ trumpvote                   â”‚           1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ missionary                  â”‚           1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ whammy                      â”‚           2 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ kip                         â”‚           1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ trumpdopa                   â”‚           1 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ brainer                     â”‚           3 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ sooo                        â”‚          14 â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ hairforceone                â”‚           1 â”‚\n",
      "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_tfidf = sort_dict(tfidf_dict)\n",
    "display_table(sorted_tfidf, title=\"Highest TF-IDF words from Tweets\", headers=['Words','TF-IDF'])\n",
    "\n",
    "# lets find out how frequent the top words are\n",
    "tfidf_words, _ = zip(*sorted_tfidf)\n",
    "data = [ [word[0], freq_dist[word[0]]] for word in sorted_tfidf]\n",
    "\n",
    "display_table(data, title=\"Highest TF-IDF words from Tweets\", headers=['Words','Frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment(tokens):\n",
    "    \n",
    "    combined_happiness = 0\n",
    "    words_with_no_sentiment = set()\n",
    "    words_sentiment_count = 0\n",
    "    \n",
    "    # Go through each token and if we have a sentiment for it, add it to the combined happiness score\n",
    "    for token in tokens:\n",
    "        # get the happiness value, otherwise return zero\n",
    "        happiest_value = words_happiness.get(token,0)\n",
    "        \n",
    "        # if a happiness value exists keep track \n",
    "        if happiest_value != 0:\n",
    "            combined_happiness += happiest_value\n",
    "            words_sentiment_count += 1\n",
    "        # save the words that have no happiness index\n",
    "        else:\n",
    "            words_with_no_sentiment.add(token)\n",
    "    \n",
    "    # Safe check to avoid division by 0\n",
    "    if combined_happiness == 0: \n",
    "        avg_sentiment_score = 0 \n",
    "    else: \n",
    "        avg_sentiment_score = combined_happiness / words_sentiment_count\n",
    "    \n",
    "    return avg_sentiment_score, words_with_no_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_sentiment = {}\n",
    "tweets_with_no_sentiment = []\n",
    "all_words_with_no_sentiment = set()\n",
    "\n",
    "for tweet in tweet_iterable(text_only=False):\n",
    "    text = tweet['text']\n",
    "    _id = tweet['_id']\n",
    "    \n",
    "    tokens = tweet_tokenizer(text)\n",
    "    \n",
    "    avg_sentiment_score, words_with_no_sentiment = get_sentiment(tokens)\n",
    "    \n",
    "    # update set of words with no sentiment\n",
    "    if len(words_with_no_sentiment) > 0:\n",
    "        all_words_with_no_sentiment = all_words_with_no_sentiment.union(words_with_no_sentiment)\n",
    "    \n",
    "    # If we didn't find any sentiment for the tweet, save the tweet id\n",
    "    if avg_sentiment_score == 0:\n",
    "        tweets_with_no_sentiment.append(_id)\n",
    "    # otherwise save the sentiment to the dictionary\n",
    "    else:\n",
    "        tweet_sentiment[_id] = avg_sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71265\n",
      "71370\n",
      "105\n",
      "26128\n"
     ]
    }
   ],
   "source": [
    "print len(tweet_sentiment)\n",
    "cur = tweet_collection.find({'retweeted': False})\n",
    "print cur.count()\n",
    "print len(tweets_with_no_sentiment)\n",
    "print len(all_words_with_no_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average sentiment of tweet sentiment is   5.57\n",
      "The stanard deviation of tweet sentiment is   0.47\n",
      "\n",
      "Percentage of tweets that have a sentiment    99.85%\n",
      "Percentage of words that have a sentiment     29.33%\n",
      "\n",
      "The happiest tweet is                         8.18\n",
      "Tweet text: @IcomOfficiel @UNESCO Excellent.\n",
      "BUT coinciding with https://t.co/2UoluNULzY\n",
      "\n",
      "The saddest tweet is                          1.56\n",
      "Tweet text: @xtremevicky \"Harambe died for this?\"\n",
      "#Elections2016 ðŸ˜‚ https://t.co/ygtf0tnjl4\n"
     ]
    }
   ],
   "source": [
    "values = tweet_sentiment.values()\n",
    "\n",
    "average_sentiment = np.average(values)\n",
    "std_sentiment = np.std(values)\n",
    "happiest_value = max(values)\n",
    "saddest_value = min(values)\n",
    "\n",
    "tweet_percent = (len(tweet_sentiment) / (len(tweets_with_no_sentiment) + len(tweet_sentiment))) * 100\n",
    "word_percent = (1 - len(all_words_with_no_sentiment) / len(unique_words)) * 100\n",
    "\n",
    "happiest_tweet_id = tweet_sentiment.keys()[values.index(happiest_value)]\n",
    "saddest_tweet_id = tweet_sentiment.keys()[values.index(saddest_value)]\n",
    "\n",
    "happy_tweet = tweet_collection.find_one({'_id': happiest_tweet_id})['text']\n",
    "sad_tweet = tweet_collection.find_one({'_id': saddest_tweet_id})['text']\n",
    "\n",
    "print \"The average sentiment of tweet sentiment is   %2.2f\" % average_sentiment\n",
    "print \"The stanard deviation of tweet sentiment is   %2.2f\" % std_sentiment\n",
    "print ''\n",
    "print \"Percentage of tweets that have a sentiment    %2.2f%%\" % tweet_percent\n",
    "print \"Percentage of words that have a sentiment     %2.2f%%\" % word_percent\n",
    "print ''\n",
    "print \"The happiest tweet is                         %2.2f\" % happiest_value\n",
    "print \"Tweet text: %s\" % happy_tweet\n",
    "print ''\n",
    "print \"The saddest tweet is                          %2.2f\" % saddest_value\n",
    "print \"Tweet text: %s\" % sad_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for w in tweet_tokenizer(happy_tweet):\n",
    "    print words_happiness.get(w,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['Tweet ID','Happiness Index']\n",
    "\n",
    "display_table(sort_dict(tweet_sentiment), title=\"Happiest Tweets\", limit=10, headers=headers)\n",
    "\n",
    "display_table(sort_dict(tweet_sentiment, reverse=False), title=\"Unhappiest Tweets\", limit=10, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'@xtremevicky \"Harambe died for this?\"\\n#Elections2016 \\U0001f602 https://t.co/ygtf0tnjl4'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sad_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
